{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd1aa827-fad5-425b-834f-50b305278f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc6589a6-435f-4109-8c3f-6f71651e17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2\n",
    "num_epochs = 100\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "# data_path = 'gdrive/My Drive/MIDA/MIDA-pytorch-master/data/BostonHousing.csv'\n",
    "mechanism = 'mcar'\n",
    "method = 'uniform'\n",
    "\n",
    "test_size = 0.3\n",
    "use_cuda = True\n",
    "batch_size  = 288 # not in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a9e7c45-e7ab-48ed-b808-9f86c774b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "data = pd.read_csv('Data_Augmented.csv')\n",
    "train_truth = data.iloc[0:69120,:]\n",
    "test_truth  = data.iloc[86400:103680,:]\n",
    "\n",
    "# standardized between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_truth)\n",
    "train_truth = scaler.transform(train_truth)\n",
    "test_truth = scaler.transform(test_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b589981-07c0-46fb-b39a-0db1785052f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.read_csv('2hr_BM.csv')\n",
    "corrupted = pd.read_csv('Data_Augmented.csv')\n",
    "\n",
    "mask_train = mask.iloc[0:69120,:]\n",
    "mask_train = torch.tensor(mask_train.values)\n",
    "train_corrupted = corrupted.iloc[0:69120,:]\n",
    "train_corrupted = scaler.transform(train_corrupted)\n",
    "train_corrupted[mask_train.numpy()==False] = 0\n",
    "\n",
    "mask_test = mask.iloc[86400:103680,:]\n",
    "mask_test = torch.tensor(mask_test.values)\n",
    "test_corrupted = corrupted.iloc[86400:103680,:]\n",
    "test_corrupted = scaler.transform(test_corrupted)\n",
    "test_corrupted[mask_test.numpy()==False] = 0\n",
    "# test_corrupted = torch.tensor(test_corrupted.values)\n",
    "\n",
    "train_corrupted = torch.from_numpy(train_corrupted).float()\n",
    "test_corrupted = torch.from_numpy(test_corrupted).float()\n",
    "train_truth = torch.from_numpy(train_truth).float()\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_truth, train_corrupted)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac94257-1451-495d-93a4-0f1cd5a54d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69120, 98])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ff7016-0db5-45f7-a55c-ed4b868ad5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = pd.read_csv('20%MCAR.csv')\n",
    "# corrupted = pd.read_csv('Pseudo_imputed.csv')\n",
    "\n",
    "\n",
    "# mask_train = mask.iloc[0:69120,:]\n",
    "# mask_train = torch.tensor(mask_train.values)\n",
    "# train_corrupted = corrupted.iloc[0:69120,:]\n",
    "# train_corrupted[mask_train.numpy()==False] = 0\n",
    "# train_corrupted = torch.tensor(train_corrupted.values)\n",
    "# train_corrupted.shape\n",
    "\n",
    "\n",
    "# mask_test = mask.iloc[86400:103680,:]\n",
    "# mask_test = torch.tensor(mask_test.values)\n",
    "# test_corrupted = corrupted.iloc[86400:103680,:]\n",
    "# test_corrupted[mask_test.numpy()==False] = 0\n",
    "# test_corrupted = torch.tensor(test_corrupted.values)\n",
    "# test_corrupted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b525975-87d2-49df-94d7-001594b20ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*0, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*3, dim+theta*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*4, dim+theta*5),\n",
    "        )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*5, dim+theta*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*4, dim+theta*3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*3, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim)\n",
    "        x_missed = self.drop_out(x)\n",
    "        \n",
    "        z = self.encoder(x_missed)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "        \n",
    "        out = out.view(-1, self.dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d6c853-d584-4a5c-8857-dc98220d9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(dim=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1830f8ba-6d4e-4279-9ce1-d5026115e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.025, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44de1fed-a7e2-4c19-833b-bbcdc3addcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], lter [120/240], Loss: 0.087414\n",
      "Epoch [1/100], lter [240/240], Loss: 0.022934\n",
      "Epoch [2/100], lter [120/240], Loss: 0.013766\n",
      "Epoch [2/100], lter [240/240], Loss: 0.011588\n",
      "Epoch [3/100], lter [120/240], Loss: 0.014395\n",
      "Epoch [3/100], lter [240/240], Loss: 0.011927\n",
      "Epoch [4/100], lter [120/240], Loss: 0.010589\n",
      "Epoch [4/100], lter [240/240], Loss: 0.011541\n",
      "Epoch [5/100], lter [120/240], Loss: 0.010661\n",
      "Epoch [5/100], lter [240/240], Loss: 0.006488\n",
      "Epoch [6/100], lter [120/240], Loss: 0.007448\n",
      "Epoch [6/100], lter [240/240], Loss: 0.006693\n",
      "Epoch [7/100], lter [120/240], Loss: 0.008091\n",
      "Epoch [7/100], lter [240/240], Loss: 0.006387\n",
      "Epoch [8/100], lter [120/240], Loss: 0.007815\n",
      "Epoch [8/100], lter [240/240], Loss: 0.007069\n",
      "Epoch [9/100], lter [120/240], Loss: 0.006672\n",
      "Epoch [9/100], lter [240/240], Loss: 0.005528\n",
      "Epoch [10/100], lter [120/240], Loss: 0.004396\n",
      "Epoch [10/100], lter [240/240], Loss: 0.004754\n",
      "Epoch [11/100], lter [120/240], Loss: 0.003480\n",
      "Epoch [11/100], lter [240/240], Loss: 0.003402\n",
      "Epoch [12/100], lter [120/240], Loss: 0.002758\n",
      "Epoch [12/100], lter [240/240], Loss: 0.002821\n",
      "Epoch [13/100], lter [120/240], Loss: 0.002834\n",
      "Epoch [13/100], lter [240/240], Loss: 0.003332\n",
      "Epoch [14/100], lter [120/240], Loss: 0.002269\n",
      "Epoch [14/100], lter [240/240], Loss: 0.001946\n",
      "Epoch [15/100], lter [120/240], Loss: 0.003195\n",
      "Epoch [15/100], lter [240/240], Loss: 0.002138\n",
      "Epoch [16/100], lter [120/240], Loss: 0.002843\n",
      "Epoch [16/100], lter [240/240], Loss: 0.001840\n",
      "Epoch [17/100], lter [120/240], Loss: 0.002972\n",
      "Epoch [17/100], lter [240/240], Loss: 0.002305\n",
      "Epoch [18/100], lter [120/240], Loss: 0.002176\n",
      "Epoch [18/100], lter [240/240], Loss: 0.002040\n",
      "Epoch [19/100], lter [120/240], Loss: 0.002364\n",
      "Epoch [19/100], lter [240/240], Loss: 0.001822\n",
      "Epoch [20/100], lter [120/240], Loss: 0.001525\n",
      "Epoch [20/100], lter [240/240], Loss: 0.001380\n",
      "Epoch [21/100], lter [120/240], Loss: 0.002145\n",
      "Epoch [21/100], lter [240/240], Loss: 0.001549\n",
      "Epoch [22/100], lter [120/240], Loss: 0.001672\n",
      "Epoch [22/100], lter [240/240], Loss: 0.001519\n",
      "Epoch [23/100], lter [120/240], Loss: 0.002183\n",
      "Epoch [23/100], lter [240/240], Loss: 0.001911\n",
      "Epoch [24/100], lter [120/240], Loss: 0.001463\n",
      "Epoch [24/100], lter [240/240], Loss: 0.002118\n",
      "Epoch [25/100], lter [120/240], Loss: 0.001802\n",
      "Epoch [25/100], lter [240/240], Loss: 0.001839\n",
      "Epoch [26/100], lter [120/240], Loss: 0.001211\n",
      "Epoch [26/100], lter [240/240], Loss: 0.001681\n",
      "Epoch [27/100], lter [120/240], Loss: 0.001589\n",
      "Epoch [27/100], lter [240/240], Loss: 0.001566\n",
      "Epoch [28/100], lter [120/240], Loss: 0.002036\n",
      "Epoch [28/100], lter [240/240], Loss: 0.001850\n",
      "Epoch [29/100], lter [120/240], Loss: 0.001510\n",
      "Epoch [29/100], lter [240/240], Loss: 0.001498\n",
      "Epoch [30/100], lter [120/240], Loss: 0.001388\n",
      "Epoch [30/100], lter [240/240], Loss: 0.001210\n",
      "Epoch [31/100], lter [120/240], Loss: 0.001325\n",
      "Epoch [31/100], lter [240/240], Loss: 0.001288\n",
      "Epoch [32/100], lter [120/240], Loss: 0.001120\n",
      "Epoch [32/100], lter [240/240], Loss: 0.001340\n",
      "Epoch [33/100], lter [120/240], Loss: 0.000945\n",
      "Epoch [33/100], lter [240/240], Loss: 0.000890\n",
      "Epoch [34/100], lter [120/240], Loss: 0.001141\n",
      "Epoch [34/100], lter [240/240], Loss: 0.001248\n",
      "Epoch [35/100], lter [120/240], Loss: 0.001288\n",
      "Epoch [35/100], lter [240/240], Loss: 0.001530\n",
      "Epoch [36/100], lter [120/240], Loss: 0.001289\n",
      "Epoch [36/100], lter [240/240], Loss: 0.001350\n",
      "Epoch [37/100], lter [120/240], Loss: 0.001198\n",
      "Epoch [37/100], lter [240/240], Loss: 0.001179\n",
      "Epoch [38/100], lter [120/240], Loss: 0.001002\n",
      "Epoch [38/100], lter [240/240], Loss: 0.001143\n",
      "Epoch [39/100], lter [120/240], Loss: 0.001479\n",
      "Epoch [39/100], lter [240/240], Loss: 0.001166\n",
      "Epoch [40/100], lter [120/240], Loss: 0.000844\n",
      "Epoch [40/100], lter [240/240], Loss: 0.001088\n",
      "Epoch [41/100], lter [120/240], Loss: 0.001057\n",
      "Epoch [41/100], lter [240/240], Loss: 0.001085\n",
      "Epoch [42/100], lter [120/240], Loss: 0.001417\n",
      "Epoch [42/100], lter [240/240], Loss: 0.000939\n",
      "Epoch [43/100], lter [120/240], Loss: 0.001090\n",
      "Epoch [43/100], lter [240/240], Loss: 0.000827\n",
      "Epoch [44/100], lter [120/240], Loss: 0.001074\n",
      "Epoch [44/100], lter [240/240], Loss: 0.001264\n",
      "Epoch [45/100], lter [120/240], Loss: 0.001083\n",
      "Epoch [45/100], lter [240/240], Loss: 0.001035\n",
      "Epoch [46/100], lter [120/240], Loss: 0.001213\n",
      "Epoch [46/100], lter [240/240], Loss: 0.001050\n",
      "Epoch [47/100], lter [120/240], Loss: 0.001130\n",
      "Epoch [47/100], lter [240/240], Loss: 0.001024\n",
      "Epoch [48/100], lter [120/240], Loss: 0.001244\n",
      "Epoch [48/100], lter [240/240], Loss: 0.001005\n",
      "Epoch [49/100], lter [120/240], Loss: 0.001399\n",
      "Epoch [49/100], lter [240/240], Loss: 0.000890\n",
      "Epoch [50/100], lter [120/240], Loss: 0.001494\n",
      "Epoch [50/100], lter [240/240], Loss: 0.001342\n",
      "Epoch [51/100], lter [120/240], Loss: 0.001343\n",
      "Epoch [51/100], lter [240/240], Loss: 0.001285\n",
      "Epoch [52/100], lter [120/240], Loss: 0.001195\n",
      "Epoch [52/100], lter [240/240], Loss: 0.000883\n",
      "Epoch [53/100], lter [120/240], Loss: 0.001082\n",
      "Epoch [53/100], lter [240/240], Loss: 0.001289\n",
      "Epoch [54/100], lter [120/240], Loss: 0.001178\n",
      "Epoch [54/100], lter [240/240], Loss: 0.001259\n",
      "Epoch [55/100], lter [120/240], Loss: 0.001008\n",
      "Epoch [55/100], lter [240/240], Loss: 0.001253\n",
      "Epoch [56/100], lter [120/240], Loss: 0.001211\n",
      "Epoch [56/100], lter [240/240], Loss: 0.000958\n",
      "Epoch [57/100], lter [120/240], Loss: 0.000852\n",
      "Epoch [57/100], lter [240/240], Loss: 0.000853\n",
      "Epoch [58/100], lter [120/240], Loss: 0.001002\n",
      "Epoch [58/100], lter [240/240], Loss: 0.001449\n",
      "Epoch [59/100], lter [120/240], Loss: 0.001111\n",
      "Epoch [59/100], lter [240/240], Loss: 0.001010\n",
      "Epoch [60/100], lter [120/240], Loss: 0.000972\n",
      "Epoch [60/100], lter [240/240], Loss: 0.000856\n",
      "Epoch [61/100], lter [120/240], Loss: 0.000831\n",
      "Epoch [61/100], lter [240/240], Loss: 0.000943\n",
      "Epoch [62/100], lter [120/240], Loss: 0.001246\n",
      "Epoch [62/100], lter [240/240], Loss: 0.000870\n",
      "Epoch [63/100], lter [120/240], Loss: 0.000939\n",
      "Epoch [63/100], lter [240/240], Loss: 0.001277\n",
      "Epoch [64/100], lter [120/240], Loss: 0.001198\n",
      "Epoch [64/100], lter [240/240], Loss: 0.001190\n",
      "Epoch [65/100], lter [120/240], Loss: 0.001574\n",
      "Epoch [65/100], lter [240/240], Loss: 0.001302\n",
      "Epoch [66/100], lter [120/240], Loss: 0.000923\n",
      "Epoch [66/100], lter [240/240], Loss: 0.001421\n",
      "Epoch [67/100], lter [120/240], Loss: 0.000891\n",
      "Epoch [67/100], lter [240/240], Loss: 0.001191\n",
      "Epoch [68/100], lter [120/240], Loss: 0.001180\n",
      "Epoch [68/100], lter [240/240], Loss: 0.001165\n",
      "Epoch [69/100], lter [120/240], Loss: 0.001113\n",
      "Epoch [69/100], lter [240/240], Loss: 0.000978\n",
      "Epoch [70/100], lter [120/240], Loss: 0.001263\n",
      "Epoch [70/100], lter [240/240], Loss: 0.001211\n",
      "Epoch [71/100], lter [120/240], Loss: 0.001004\n",
      "Epoch [71/100], lter [240/240], Loss: 0.000855\n",
      "Epoch [72/100], lter [120/240], Loss: 0.000931\n",
      "Epoch [72/100], lter [240/240], Loss: 0.001101\n",
      "Epoch [73/100], lter [120/240], Loss: 0.001029\n",
      "Epoch [73/100], lter [240/240], Loss: 0.000881\n",
      "Epoch [74/100], lter [120/240], Loss: 0.001305\n",
      "Epoch [74/100], lter [240/240], Loss: 0.000891\n",
      "Epoch [75/100], lter [120/240], Loss: 0.001006\n",
      "Epoch [75/100], lter [240/240], Loss: 0.000803\n",
      "Epoch [76/100], lter [120/240], Loss: 0.000986\n",
      "Epoch [76/100], lter [240/240], Loss: 0.000970\n",
      "Epoch [77/100], lter [120/240], Loss: 0.001074\n",
      "Epoch [77/100], lter [240/240], Loss: 0.001080\n",
      "Epoch [78/100], lter [120/240], Loss: 0.001001\n",
      "Epoch [78/100], lter [240/240], Loss: 0.001361\n",
      "Epoch [79/100], lter [120/240], Loss: 0.000971\n",
      "Epoch [79/100], lter [240/240], Loss: 0.000962\n",
      "Epoch [80/100], lter [120/240], Loss: 0.001226\n",
      "Epoch [80/100], lter [240/240], Loss: 0.001068\n",
      "Epoch [81/100], lter [120/240], Loss: 0.001075\n",
      "Epoch [81/100], lter [240/240], Loss: 0.000940\n",
      "Epoch [82/100], lter [120/240], Loss: 0.000972\n",
      "Epoch [82/100], lter [240/240], Loss: 0.001265\n",
      "Epoch [83/100], lter [120/240], Loss: 0.000766\n",
      "Epoch [83/100], lter [240/240], Loss: 0.000870\n",
      "Epoch [84/100], lter [120/240], Loss: 0.000913\n",
      "Epoch [84/100], lter [240/240], Loss: 0.000781\n",
      "Epoch [85/100], lter [120/240], Loss: 0.001244\n",
      "Epoch [85/100], lter [240/240], Loss: 0.000873\n",
      "Epoch [86/100], lter [120/240], Loss: 0.001083\n",
      "Epoch [86/100], lter [240/240], Loss: 0.001266\n",
      "Epoch [87/100], lter [120/240], Loss: 0.001126\n",
      "Epoch [87/100], lter [240/240], Loss: 0.001022\n",
      "Epoch [88/100], lter [120/240], Loss: 0.001214\n",
      "Epoch [88/100], lter [240/240], Loss: 0.001213\n",
      "Epoch [89/100], lter [120/240], Loss: 0.001061\n",
      "Epoch [89/100], lter [240/240], Loss: 0.000911\n",
      "Epoch [90/100], lter [120/240], Loss: 0.000973\n",
      "Epoch [90/100], lter [240/240], Loss: 0.000896\n",
      "Epoch [91/100], lter [120/240], Loss: 0.000899\n",
      "Epoch [91/100], lter [240/240], Loss: 0.000949\n",
      "Epoch [92/100], lter [120/240], Loss: 0.000922\n",
      "Epoch [92/100], lter [240/240], Loss: 0.000931\n",
      "Epoch [93/100], lter [120/240], Loss: 0.000833\n",
      "Epoch [93/100], lter [240/240], Loss: 0.001283\n",
      "Epoch [94/100], lter [120/240], Loss: 0.001218\n",
      "Epoch [94/100], lter [240/240], Loss: 0.001196\n",
      "Epoch [95/100], lter [120/240], Loss: 0.000945\n",
      "Epoch [95/100], lter [240/240], Loss: 0.001065\n",
      "Epoch [96/100], lter [120/240], Loss: 0.000935\n",
      "Epoch [96/100], lter [240/240], Loss: 0.001035\n",
      "Epoch [97/100], lter [120/240], Loss: 0.001232\n",
      "Epoch [97/100], lter [240/240], Loss: 0.001104\n",
      "Epoch [98/100], lter [120/240], Loss: 0.000906\n",
      "Epoch [98/100], lter [240/240], Loss: 0.001034\n",
      "Epoch [99/100], lter [120/240], Loss: 0.000853\n",
      "Epoch [99/100], lter [240/240], Loss: 0.000920\n",
      "Epoch [100/100], lter [120/240], Loss: 0.001091\n",
      "Epoch [100/100], lter [240/240], Loss: 0.001050\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_list = []\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        train_truth, train_corrupted = batch_data\n",
    "        \n",
    "        reconst_data = model(train_corrupted)\n",
    "        cost = loss(reconst_data, train_truth)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if (i+1) % (total_batch//2) == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "            \n",
    "        # early stopping rule 1 : MSE < 1e-06\n",
    "        if cost.item() < 1e-06 :\n",
    "            early_stop = True\n",
    "            break\n",
    "            \n",
    "#         early stopping rule 2 : simple moving average of length 5\n",
    "#         sometimes it doesn't work well.\n",
    "#         if len(cost_list) > 5 :\n",
    "#            if cost.item() > np.mean(cost_list[-5:]):\n",
    "#                early_stop = True\n",
    "#                break\n",
    "                \n",
    "        cost_list.append(cost.item())\n",
    "\n",
    "    if early_stop :\n",
    "        break\n",
    "        \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70831a82-1390-4405-88ae-c7afbb88db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d461d80-868d-4ea2-9311-71db17e5087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(test_corrupted.float())\n",
    "pred = pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9402a748-3940-4ab8-9036-f72377d7ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler.inverse_transform(pred)\n",
    "test_truth = scaler.inverse_transform(test_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689f9467-2e5a-4067-aa04-58d438295faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE of MIDA is: 2.7102901905690207\n",
      "Test RMSE of MIDA is: 5.881549388590342\n"
     ]
    }
   ],
   "source": [
    "MIDA_pred = pred[mask_test.numpy()==False]\n",
    "\n",
    "ground_truth = test_truth[mask_test.numpy()==False]\n",
    "print(\"Test MAE of MIDA is: \"+str(np.mean(abs(MIDA_pred-ground_truth))))\n",
    "print(\"Test RMSE of MIDA is: \"+str(sqrt(mean_squared_error(MIDA_pred, ground_truth))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "361aca80-6e84-4d24-b3cf-0d9476edb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtr = pd.date_range(start='2016-06-27', end='2016-08-26', freq='5min')\n",
    "\n",
    "pred = pd.DataFrame(pred)\n",
    "pred['tmst']=dtr[0:17280]\n",
    "pred.to_csv('./Model_results/12hrBM_MIDA_Imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711ccc2-06f8-4d55-a59f-b42a2ede0d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
