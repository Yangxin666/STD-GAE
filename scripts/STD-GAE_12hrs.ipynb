{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10qCANVbEbPm",
   "metadata": {
    "id": "10qCANVbEbPm"
   },
   "source": [
    "# Spatio-Temporal Denoising Graph Autoencoder (STD-GAE) \n",
    "\n",
    "STD-GAE exploits temporal correlation, spatial coherence and value dependencies from domain knowledge to recover missing data. It is empowered by two modules. \n",
    "\n",
    "1.   To cope with sparse yet various scenarios of missing data, STD-GAE incorporates a domain-knowledge aware data augmentation module\n",
    "that creates plausible variations of missing data patterns. This generalizes\n",
    "STD-GAE to robust imputation over different seasons and\n",
    "environment.\n",
    "2.   STD-GAE nontrivially integrates spatiotemporal\n",
    "graph convolution layers (to recover local missing data by observed\n",
    "“neighboring” PV plants) and denoising autoencoder (to recover\n",
    "corrupted data from augmented counterpart) to improve the accu-\n",
    "racy of imputation accuracy at PV fleet level. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be959fc3-bc58-4f14-86ac-cddf863a6c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/axn392/gnn',\n",
       " '',\n",
       " '/usr/local/lib/python3.8/dist-packages',\n",
       " '/home/rxf131/ondemand/ubuntu2004/torch-geometric-temporal',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '/usr/lib/python3/dist-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path = ['/home/axn392/gnn',\n",
    " '',\n",
    " '/usr/local/lib/python3.8/dist-packages',\n",
    " '/home/rxf131/ondemand/ubuntu2004/torch-geometric-temporal',\n",
    " '/usr/lib/python38.zip',\n",
    " '/usr/lib/python3.8',\n",
    " '/usr/lib/python3.8/lib-dynload',\n",
    " '/usr/lib/python3/dist-packages']\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6864f00-17c3-4b8a-a066-808dbd256613",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6864f00-17c3-4b8a-a066-808dbd256613",
    "outputId": "02d4d2cb-eb57-41a0-a4d9-a352ef7f7865"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxf131/ondemand/ubuntu2004/torch-geometric-temporal/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#import geopy.distance # to compute distances between stations\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch_geometric_temporal.nn import STConv\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mYgyw7lt2FQN",
   "metadata": {
    "id": "mYgyw7lt2FQN"
   },
   "source": [
    "#Generate Edge Weight Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ENvs9ivC2KUD",
   "metadata": {
    "id": "ENvs9ivC2KUD"
   },
   "source": [
    "We represent the spatiotemporal PV data as an undirected graph $G = (V, E, X_{t})$. \n",
    "\n",
    "1.   Each node in $V$ represents a PV inverter\n",
    "2.   Edges $E$ are assigned according to Edge Weight Matrix (if $W_{i,j}$ > 0, then there is edge between i and j)\n",
    "3.   $X_{t}$ denotes a node attribute tensor $\\in \\mathbb{R}^{T\\times n\\times d}$. Here T is the length of timeseries, $n$ is the number of nodes (which is 98 in our study), and $d$ is the number of input channel.\n",
    "\n",
    "Since the locations of PV inverters are fixed, the graph structure is static with time-invariant nodes and edges. \n",
    "However, $X_{t}$ is time-varying: each node i carries a  timeseries $x_{i} \\in \\mathbb{R}^{T\\times d}$ recording attributes \n",
    "such as temperature, wind speed, \n",
    "irradiance and power output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "XuqTiBY_2IRh",
   "metadata": {
    "id": "XuqTiBY_2IRh"
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qYGJu5AN2IXB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "qYGJu5AN2IXB",
    "outputId": "616e6209-a5de-4ca3-f2e4-914b58318600"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>22.162095</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "      <td>32.273508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>10.142698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5   \\\n",
       "0    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "12  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "13  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "14  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "15  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "16  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "17  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "18  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "19  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "20  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "21  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "22  22.162095  22.162095  22.162095  22.162095  22.162095  22.162095   \n",
       "23  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "24  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "25  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "26  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "27  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "28  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "29  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "30  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "31  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "32  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "33  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "34  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "\n",
       "           6          7          8          9   ...         25         26  \\\n",
       "0    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "1    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "2    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "3    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "4    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "5    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "6    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "7    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "8    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "9    0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "10   0.000000   0.000000   0.000000   0.000000  ...  10.142698  10.142698   \n",
       "11  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "12  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "13  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "14  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "15  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "16  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "17  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "18  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "19  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "20  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "21  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "22  22.162095  22.162095  22.162095  22.162095  ...  32.273508  32.273508   \n",
       "23  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "24  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "25  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "26  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "27  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "28  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "29  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "30  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "31  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "32  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "33  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "34  10.142698  10.142698  10.142698  10.142698  ...   0.000000   0.000000   \n",
       "\n",
       "           27         28         29         30         31         32  \\\n",
       "0   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "1   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "2   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "3   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "4   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "5   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "6   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "7   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "8   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "9   10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "10  10.142698  10.142698  10.142698  10.142698  10.142698  10.142698   \n",
       "11  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "12  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "13  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "14  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "15  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "16  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "17  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "18  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "19  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "20  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "21  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "22  32.273508  32.273508  32.273508  32.273508  32.273508  32.273508   \n",
       "23   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "24   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "25   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "26   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "27   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "28   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "29   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "30   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "31   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "32   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "33   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "34   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "           33         34  \n",
       "0   10.142698  10.142698  \n",
       "1   10.142698  10.142698  \n",
       "2   10.142698  10.142698  \n",
       "3   10.142698  10.142698  \n",
       "4   10.142698  10.142698  \n",
       "5   10.142698  10.142698  \n",
       "6   10.142698  10.142698  \n",
       "7   10.142698  10.142698  \n",
       "8   10.142698  10.142698  \n",
       "9   10.142698  10.142698  \n",
       "10  10.142698  10.142698  \n",
       "11  32.273508  32.273508  \n",
       "12  32.273508  32.273508  \n",
       "13  32.273508  32.273508  \n",
       "14  32.273508  32.273508  \n",
       "15  32.273508  32.273508  \n",
       "16  32.273508  32.273508  \n",
       "17  32.273508  32.273508  \n",
       "18  32.273508  32.273508  \n",
       "19  32.273508  32.273508  \n",
       "20  32.273508  32.273508  \n",
       "21  32.273508  32.273508  \n",
       "22  32.273508  32.273508  \n",
       "23   0.000000   0.000000  \n",
       "24   0.000000   0.000000  \n",
       "25   0.000000   0.000000  \n",
       "26   0.000000   0.000000  \n",
       "27   0.000000   0.000000  \n",
       "28   0.000000   0.000000  \n",
       "29   0.000000   0.000000  \n",
       "30   0.000000   0.000000  \n",
       "31   0.000000   0.000000  \n",
       "32   0.000000   0.000000  \n",
       "33   0.000000   0.000000  \n",
       "34   0.000000   0.000000  \n",
       "\n",
       "[35 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# please change the path according to your setting\n",
    "location = pd.read_csv('location_35.csv',index_col=0)\n",
    "distance = np.zeros(shape=(35,35))\n",
    "dist = []\n",
    "for i in range(35):\n",
    "    for j in range(35):\n",
    "        d = haversine(location.iloc[i][1], location.iloc[i][0], location.iloc[j][1], location.iloc[j][0])\n",
    "        distance[i][j] = d\n",
    "        dist.append(d)\n",
    "\n",
    "dist_std = np.std(dist)\n",
    "distance = pd.DataFrame(distance)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gI2ZfOe13ZS_",
   "metadata": {
    "id": "gI2ZfOe13ZS_"
   },
   "source": [
    "In the next step, we compute $w_{ij}$ for each pair of nodes $(i, j)$ in the graph as presented in Yu et al. (2018):\n",
    "\n",
    "$W_{i,j} = \\begin{cases} \\exp{(-\\frac{d_{ij}^2}{\\sigma^2})}, i\\neq j \\text{ and } \\exp{(-\\frac{d_{ij}^2}{\\sigma^2})} \\geq \\epsilon \\\\ \n",
    "0 \\text{ otherwise }\n",
    "\\end{cases}$\n",
    "\n",
    "In this case, $d_{ij}$ for each pair of nodes $(i, j)$ is the distance between stations that we have computed above. $\\sigma$ is the standard deviation of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "JEC0_3PE2IZ4",
   "metadata": {
    "id": "JEC0_3PE2IZ4"
   },
   "outputs": [],
   "source": [
    "# epsilon = 0, 0.25, 0.5, 0.75, 1\n",
    "epsilon = 1\n",
    "sigma = dist_std\n",
    "W = np.zeros(shape=(35,35))\n",
    "\n",
    "for i in range(35):\n",
    "    for j in range(35):\n",
    "        if i == j: \n",
    "            W[i][j] = 0\n",
    "        else:\n",
    "            # Compute distance between stations\n",
    "            d_ij = distance.loc[i][j]\n",
    "            \n",
    "            # Compute weight w_ij\n",
    "            w_ij = np.exp(-d_ij**2 / sigma**2)\n",
    "            \n",
    "            if w_ij >= epsilon:\n",
    "                W[i, j] = w_ij\n",
    "\n",
    "W = pd.DataFrame(W)\n",
    "# please change the path according to your setting\n",
    "W.to_csv('W_35.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NdGpxGEInQMM",
   "metadata": {
    "id": "NdGpxGEInQMM"
   },
   "source": [
    "#Construction of STD-GAE \n",
    "\n",
    "\n",
    "*   Construct temporal & spatial layers -> ST Blocks (encoder & decoder) -> Graph Denoising Autoencoders \n",
    "*   Define Parameters\n",
    "*   Some utility functions\n",
    "\n",
    "\n",
    "Construction of STD-GAE is the foundation for later codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f1baf-a9e2-4f21-a62e-cf6e44cffcc6",
   "metadata": {
    "id": "210f1baf-a9e2-4f21-a62e-cf6e44cffcc6"
   },
   "source": [
    "Temporal Convolutional (TConv) Layers and Deconvolutional (DeConv) Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9668e94c-7e80-4961-9b3f-e9cb1f6dd6ff",
   "metadata": {
    "id": "9668e94c-7e80-4961-9b3f-e9cb1f6dd6ff"
   },
   "outputs": [],
   "source": [
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int, padding: int):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Forward pass through temporal convolution block.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (torch.FloatTensor) -  Input data of shape\n",
    "                (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Return types:\n",
    "            * **H** (torch.FloatTensor) - Output data of shape\n",
    "                (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "class TemporalDeConv1(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int, padding: int):\n",
    "        super(TemporalDeConv1, self).__init__()\n",
    "        self.conv_1 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_2 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride), (0,padding))\n",
    "        self.conv_3 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride), (0,padding))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Forward pass through temporal convolution block.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (torch.FloatTensor) -  Input data of shape\n",
    "                (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Return types:\n",
    "            * **H** (torch.FloatTensor) - Output data of shape\n",
    "                (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "class TemporalDeConv2(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int):\n",
    "        super(TemporalDeConv2, self).__init__()\n",
    "        self.conv_1 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "        self.conv_2 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "        self.conv_3 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Forward pass through temporal convolution block.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (torch.FloatTensor) -  Input data of shape\n",
    "                (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Return types:\n",
    "            * **H** (torch.FloatTensor) - Output data of shape\n",
    "                (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c334b5-77e4-48fa-80eb-1789c838b76d",
   "metadata": {
    "id": "c2c334b5-77e4-48fa-80eb-1789c838b76d"
   },
   "source": [
    "Encoder: a Spaio-temporal Block (Temporal Conv + Spatial Conv + Temporal Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba35dfd-375a-4735-be9d-39427fabf17f",
   "metadata": {
    "id": "8ba35dfd-375a-4735-be9d-39427fabf17f"
   },
   "outputs": [],
   "source": [
    "class STConvEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(STConvEncoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "\n",
    "        self._temporal_conv1 = TemporalConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "\n",
    "        self._graph_conv = ChebConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            K=K,\n",
    "            normalization=normalization,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self._temporal_conv2 = TemporalConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "\n",
    "        self._batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,) -> torch.FloatTensor:\n",
    "\n",
    "        r\"\"\"Forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (PyTorch FloatTensor) - Sequence of node features of shape (Batch size X Input time steps X Num nodes X In channels).\n",
    "            * **edge_index** (PyTorch LongTensor) - Graph edge indices.\n",
    "            * **edge_weight** (PyTorch LongTensor, optional)- Edge weight vector.\n",
    "\n",
    "        Return types:\n",
    "            * **T** (PyTorch FloatTensor) - Sequence of node features.\n",
    "        \"\"\"\n",
    "        #print(X.shape)\n",
    "        T_0 = self._temporal_conv1(X)\n",
    "        #print(T_0.shape)\n",
    "        T = torch.zeros_like(T_0).to(T_0.device)\n",
    "        for b in range(T_0.size(0)):\n",
    "            for t in range(T_0.size(1)):\n",
    "                T[b][t] = self._graph_conv(T_0[b][t], edge_index, edge_weight)\n",
    "\n",
    "        T = F.relu(T)\n",
    "        #print(T.shape)\n",
    "        T = self._temporal_conv2(T)\n",
    "        #print(T.shape)\n",
    "        # T = T.permute(0, 2, 1, 3)\n",
    "        # #print(T.shape)\n",
    "        # T = self._batch_norm(T)\n",
    "        # T = T.permute(0, 2, 1, 3)\n",
    "        #print(T.shape)\n",
    "\n",
    "        return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fIWM0QSHxYGW",
   "metadata": {
    "id": "fIWM0QSHxYGW"
   },
   "source": [
    "Decoder: a Spaio-temporal Block (Temporal DeConv + Spatial Conv + Temporal DeConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6b79b0-4633-4b1f-8dfc-27457007e78d",
   "metadata": {
    "id": "0f6b79b0-4633-4b1f-8dfc-27457007e78d"
   },
   "outputs": [],
   "source": [
    "class STConvDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        kernel_size_de: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(STConvDecoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "\n",
    "        self._temporal_conv1 = TemporalDeConv1(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "\n",
    "        self._graph_conv = ChebConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            K=K,\n",
    "            normalization=normalization,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self._temporal_conv2 = TemporalDeConv2(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size_de, stride = stride,\n",
    "        )\n",
    "\n",
    "        self._batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,) -> torch.FloatTensor:\n",
    "\n",
    "        r\"\"\"Forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (PyTorch FloatTensor) - Sequence of node features of shape (Batch size X Input time steps X Num nodes X In channels).\n",
    "            * **edge_index** (PyTorch LongTensor) - Graph edge indices.\n",
    "            * **edge_weight** (PyTorch LongTensor, optional)- Edge weight vector.\n",
    "\n",
    "        Return types:\n",
    "            * **T** (PyTorch FloatTensor) - Sequence of node features.\n",
    "        \"\"\"\n",
    "        T_0 = self._temporal_conv1(X)\n",
    "        T = torch.zeros_like(T_0).to(T_0.device)\n",
    "        for b in range(T_0.size(0)):\n",
    "            for t in range(T_0.size(1)):\n",
    "                T[b][t] = self._graph_conv(T_0[b][t], edge_index, edge_weight)\n",
    "\n",
    "        T = F.relu(T)\n",
    "        T = self._temporal_conv2(T)\n",
    "        # T = T.permute(0, 2, 1, 3)\n",
    "        # T = self._batch_norm(T)\n",
    "        # T = T.permute(0, 2, 1, 3)\n",
    "        return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd233f-67bd-400a-81ac-8f9c80e29e52",
   "metadata": {
    "id": "ddbd233f-67bd-400a-81ac-8f9c80e29e52"
   },
   "source": [
    "Denoising Graph Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a43c31c-99b8-4edc-a6b2-ef29b20eef91",
   "metadata": {
    "id": "3a43c31c-99b8-4edc-a6b2-ef29b20eef91"
   },
   "outputs": [],
   "source": [
    "# a specified number of STConv blocks, followed by an output layer\n",
    "class STConvAE(torch.nn.Module):\n",
    "    def __init__(self, device, num_nodes, channel_size_list, num_layers, \n",
    "                 kernel_size, K, window_size, kernel_size_de, stride, padding,\\\n",
    "                 normalization = 'sym', bias = True):\n",
    "    # num_nodes = number of nodes in the input graph\n",
    "    # channel_size_list =  2d array representing feature dimensions throughout the model\n",
    "    # num_layers = number of STConv blocks\n",
    "    # kernel_size = length of the temporal kernel\n",
    "    # K = size of the chebyshev filter for the spatial convolution\n",
    "    # window_size = number of historical time steps to consider\n",
    "\n",
    "        super(STConvAE, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        # add STConv blocks\n",
    "        for l in range(num_layers):\n",
    "            input_size, hidden_size, output_size = channel_size_list[l][0], channel_size_list[l][1], channel_size_list[l][2]\n",
    "            if l==0:\n",
    "                self.layers.append(STConvEncoder(num_nodes, input_size, hidden_size, output_size, kernel_size, stride, padding, K, normalization, bias))\n",
    "            if l==1:\n",
    "                self.layers.append(STConvDecoder(num_nodes, input_size, hidden_size, output_size, kernel_size, kernel_size_de, stride, padding, K, normalization, bias))\n",
    "        \n",
    "\n",
    "        # # add output layer\n",
    "        # self.layers.append(OutputLayer(channel_size_list[-1][-1], \\\n",
    "        #                                window_size - 2 * num_layers * (kernel_size - 1), \\\n",
    "        #                                num_nodes))\n",
    "        # CUDA if available\n",
    "        for layer in self.layers:\n",
    "            layer = layer.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight ):\n",
    "        #print(x.shape)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_weight)\n",
    "          #print(x.shape)\n",
    "        # out_layer = self.layers[-1]\n",
    "        # x = x.permute(0, 3, 1, 2)\n",
    "        # x = out_layer(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffaeb1-4554-4646-ba19-84ffeffdef0a",
   "metadata": {
    "id": "71ffaeb1-4554-4646-ba19-84ffeffdef0a"
   },
   "source": [
    "Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee905283-33e3-496f-b1f0-317e3bd7e4ef",
   "metadata": {
    "id": "ee905283-33e3-496f-b1f0-317e3bd7e4ef"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "num_nodes = 35\n",
    "#channels = np.array([[1, 1, 1], [1, 1, 1]]) # sequence of channel sizes\n",
    "channels = np.array([[1, 8, 16], [16, 8, 1]])\n",
    "kernel_size = 4 # size of temporal kernel\n",
    "kernel_size_de = 2 # size of temporal deconv2\n",
    "stride = 2\n",
    "padding = 1\n",
    "K = 3 # chebyshev filter size\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "num_epochs = 50 # note that we trained for 7 epochs using Google Cloud\n",
    "num_layers = 2 # number of STConv blocks\n",
    "n_his = 288 # window size\n",
    "train_prop = 2/3 # Our actual training set proportion was 0.7\n",
    "val_prop = 1/6 # Our actual training set proportion was 0.2\n",
    "test_prop = 1/6 # Our actual training set proportion was 0.1\n",
    "\n",
    "# model save path\n",
    "model_save_path = os.path.join('best_model_12hr_BM.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c71a7d-988b-4f92-8ad9-ca54bc8a2409",
   "metadata": {
    "id": "c7c71a7d-988b-4f92-8ad9-ca54bc8a2409"
   },
   "source": [
    "Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618319d2-6ae2-448e-9dd4-b612eb239d18",
   "metadata": {
    "id": "618319d2-6ae2-448e-9dd4-b612eb239d18"
   },
   "outputs": [],
   "source": [
    "def data_transform(data, corrupted_data, window, device):\n",
    "    # data = slice of V matrix\n",
    "    # n_his = number of historical speed observations to consider\n",
    "    # n_pred = number of time steps in the future to predict\n",
    "\n",
    "    num_nodes = data.shape[1]\n",
    "    num_obs = int(len(data)/window)\n",
    "    x = np.zeros([num_obs, window, num_nodes, 1])\n",
    "    y = np.zeros([num_obs, window, num_nodes, 1])\n",
    "    \n",
    "    obs_idx = 0\n",
    "    for i in range(num_obs):\n",
    "        head = i*window\n",
    "        tail = (i+1)*window\n",
    "        y[obs_idx, :, :, :] = data[head: tail].reshape(n_his, num_nodes, 1)\n",
    "        x[obs_idx, :, :, :] = corrupted_data[head: tail].reshape(n_his, num_nodes, 1)\n",
    "        #x[obs_idx, :, :, :] = data[head: tail].reshape(n_his, num_nodes, 1)\n",
    "        obs_idx += 1\n",
    "\n",
    "    return torch.Tensor(x).to(device), torch.Tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WfWIq-pD5gln",
   "metadata": {
    "id": "WfWIq-pD5gln"
   },
   "source": [
    "#STD-GAE Framework\n",
    "STD-GAE framework consists of the following four major components: data ingestion, data augmentation, data corruption, and STD-GAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7K-ztQofpCUb",
   "metadata": {
    "id": "7K-ztQofpCUb"
   },
   "source": [
    "#Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JRfhd9K4oRf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "JRfhd9K4oRf2",
    "outputId": "ee8effcb-48f4-4c1e-8a42-fbacc34830bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>103670</th>\n",
       "      <th>103671</th>\n",
       "      <th>103672</th>\n",
       "      <th>103673</th>\n",
       "      <th>103674</th>\n",
       "      <th>103675</th>\n",
       "      <th>103676</th>\n",
       "      <th>103677</th>\n",
       "      <th>103678</th>\n",
       "      <th>103679</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.612245</td>\n",
       "      <td>23.771429</td>\n",
       "      <td>18.341497</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>15.587755</td>\n",
       "      <td>16.447619</td>\n",
       "      <td>10.442177</td>\n",
       "      <td>12.428571</td>\n",
       "      <td>6.717007</td>\n",
       "      <td>...</td>\n",
       "      <td>43.736054</td>\n",
       "      <td>42.488435</td>\n",
       "      <td>41.051701</td>\n",
       "      <td>39.389116</td>\n",
       "      <td>37.186395</td>\n",
       "      <td>35.801361</td>\n",
       "      <td>33.794558</td>\n",
       "      <td>18.436735</td>\n",
       "      <td>28.198639</td>\n",
       "      <td>14.804082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.749660</td>\n",
       "      <td>23.971429</td>\n",
       "      <td>18.451701</td>\n",
       "      <td>3.868027</td>\n",
       "      <td>4.624490</td>\n",
       "      <td>15.868027</td>\n",
       "      <td>16.703401</td>\n",
       "      <td>10.451701</td>\n",
       "      <td>12.578231</td>\n",
       "      <td>6.692517</td>\n",
       "      <td>...</td>\n",
       "      <td>44.696599</td>\n",
       "      <td>43.454422</td>\n",
       "      <td>41.942857</td>\n",
       "      <td>40.287075</td>\n",
       "      <td>38.038095</td>\n",
       "      <td>36.454422</td>\n",
       "      <td>34.304762</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>28.595918</td>\n",
       "      <td>15.008163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.725170</td>\n",
       "      <td>24.070748</td>\n",
       "      <td>18.303401</td>\n",
       "      <td>3.914286</td>\n",
       "      <td>4.012245</td>\n",
       "      <td>16.587755</td>\n",
       "      <td>16.337415</td>\n",
       "      <td>10.853061</td>\n",
       "      <td>12.319728</td>\n",
       "      <td>6.722449</td>\n",
       "      <td>...</td>\n",
       "      <td>44.721088</td>\n",
       "      <td>43.390476</td>\n",
       "      <td>41.944218</td>\n",
       "      <td>40.225850</td>\n",
       "      <td>38.035374</td>\n",
       "      <td>36.477551</td>\n",
       "      <td>34.224490</td>\n",
       "      <td>18.157823</td>\n",
       "      <td>29.416327</td>\n",
       "      <td>15.093878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.780952</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>19.444898</td>\n",
       "      <td>3.891156</td>\n",
       "      <td>4.248980</td>\n",
       "      <td>17.557823</td>\n",
       "      <td>18.242177</td>\n",
       "      <td>10.834014</td>\n",
       "      <td>13.568707</td>\n",
       "      <td>6.640816</td>\n",
       "      <td>...</td>\n",
       "      <td>43.934694</td>\n",
       "      <td>42.791837</td>\n",
       "      <td>41.670748</td>\n",
       "      <td>39.816327</td>\n",
       "      <td>37.857143</td>\n",
       "      <td>36.507483</td>\n",
       "      <td>34.556463</td>\n",
       "      <td>17.353741</td>\n",
       "      <td>30.627211</td>\n",
       "      <td>15.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.457143</td>\n",
       "      <td>24.828571</td>\n",
       "      <td>19.061224</td>\n",
       "      <td>3.869388</td>\n",
       "      <td>4.195918</td>\n",
       "      <td>17.287075</td>\n",
       "      <td>17.944218</td>\n",
       "      <td>10.682993</td>\n",
       "      <td>13.353741</td>\n",
       "      <td>6.696599</td>\n",
       "      <td>...</td>\n",
       "      <td>43.980952</td>\n",
       "      <td>42.767347</td>\n",
       "      <td>41.289796</td>\n",
       "      <td>39.642177</td>\n",
       "      <td>37.447619</td>\n",
       "      <td>36.341497</td>\n",
       "      <td>34.293878</td>\n",
       "      <td>17.238095</td>\n",
       "      <td>30.289796</td>\n",
       "      <td>15.117007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.548299</td>\n",
       "      <td>23.785034</td>\n",
       "      <td>18.357823</td>\n",
       "      <td>3.948299</td>\n",
       "      <td>5.053061</td>\n",
       "      <td>15.444898</td>\n",
       "      <td>16.398639</td>\n",
       "      <td>10.702041</td>\n",
       "      <td>12.380952</td>\n",
       "      <td>6.741497</td>\n",
       "      <td>...</td>\n",
       "      <td>44.436735</td>\n",
       "      <td>42.955102</td>\n",
       "      <td>41.545578</td>\n",
       "      <td>39.817687</td>\n",
       "      <td>37.710204</td>\n",
       "      <td>36.108844</td>\n",
       "      <td>33.960544</td>\n",
       "      <td>18.186395</td>\n",
       "      <td>28.808163</td>\n",
       "      <td>14.918367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.268027</td>\n",
       "      <td>24.627211</td>\n",
       "      <td>18.546939</td>\n",
       "      <td>3.838095</td>\n",
       "      <td>4.099320</td>\n",
       "      <td>16.644898</td>\n",
       "      <td>16.937415</td>\n",
       "      <td>10.206803</td>\n",
       "      <td>12.495238</td>\n",
       "      <td>6.786395</td>\n",
       "      <td>...</td>\n",
       "      <td>42.507483</td>\n",
       "      <td>35.410884</td>\n",
       "      <td>26.703401</td>\n",
       "      <td>19.514286</td>\n",
       "      <td>13.745578</td>\n",
       "      <td>11.927891</td>\n",
       "      <td>10.303401</td>\n",
       "      <td>7.288435</td>\n",
       "      <td>9.053061</td>\n",
       "      <td>7.529252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24.129252</td>\n",
       "      <td>22.546939</td>\n",
       "      <td>17.331973</td>\n",
       "      <td>3.643537</td>\n",
       "      <td>4.127891</td>\n",
       "      <td>14.978231</td>\n",
       "      <td>15.704762</td>\n",
       "      <td>9.718367</td>\n",
       "      <td>11.963265</td>\n",
       "      <td>6.187755</td>\n",
       "      <td>...</td>\n",
       "      <td>42.770068</td>\n",
       "      <td>41.612245</td>\n",
       "      <td>40.208163</td>\n",
       "      <td>38.642177</td>\n",
       "      <td>36.843537</td>\n",
       "      <td>35.292517</td>\n",
       "      <td>33.368707</td>\n",
       "      <td>16.865306</td>\n",
       "      <td>29.711565</td>\n",
       "      <td>14.817687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.778231</td>\n",
       "      <td>23.959184</td>\n",
       "      <td>18.512925</td>\n",
       "      <td>3.956463</td>\n",
       "      <td>4.779592</td>\n",
       "      <td>15.725170</td>\n",
       "      <td>16.526531</td>\n",
       "      <td>10.544218</td>\n",
       "      <td>12.523810</td>\n",
       "      <td>6.729252</td>\n",
       "      <td>...</td>\n",
       "      <td>44.179592</td>\n",
       "      <td>42.805442</td>\n",
       "      <td>41.390476</td>\n",
       "      <td>39.629932</td>\n",
       "      <td>37.440816</td>\n",
       "      <td>36.023129</td>\n",
       "      <td>34.048980</td>\n",
       "      <td>18.526531</td>\n",
       "      <td>28.393197</td>\n",
       "      <td>14.982313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25.840816</td>\n",
       "      <td>24.195918</td>\n",
       "      <td>18.447619</td>\n",
       "      <td>3.881633</td>\n",
       "      <td>4.093878</td>\n",
       "      <td>16.447619</td>\n",
       "      <td>16.834014</td>\n",
       "      <td>10.232653</td>\n",
       "      <td>12.440816</td>\n",
       "      <td>6.797279</td>\n",
       "      <td>...</td>\n",
       "      <td>44.839456</td>\n",
       "      <td>43.468027</td>\n",
       "      <td>41.910204</td>\n",
       "      <td>40.216327</td>\n",
       "      <td>37.910204</td>\n",
       "      <td>36.499320</td>\n",
       "      <td>34.387755</td>\n",
       "      <td>17.778231</td>\n",
       "      <td>29.835374</td>\n",
       "      <td>15.106122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26.534694</td>\n",
       "      <td>24.891156</td>\n",
       "      <td>19.244898</td>\n",
       "      <td>3.831293</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>17.991837</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>13.492517</td>\n",
       "      <td>6.553741</td>\n",
       "      <td>...</td>\n",
       "      <td>44.080272</td>\n",
       "      <td>42.866667</td>\n",
       "      <td>41.466667</td>\n",
       "      <td>39.899320</td>\n",
       "      <td>37.907483</td>\n",
       "      <td>36.542857</td>\n",
       "      <td>34.610884</td>\n",
       "      <td>17.136054</td>\n",
       "      <td>30.808163</td>\n",
       "      <td>15.153741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>17.427362</td>\n",
       "      <td>...</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "      <td>35.471421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>18.158054</td>\n",
       "      <td>...</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "      <td>38.748870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>18.328894</td>\n",
       "      <td>...</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "      <td>39.104379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>18.101834</td>\n",
       "      <td>...</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "      <td>38.542849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>17.846282</td>\n",
       "      <td>...</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "      <td>38.131652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>17.491713</td>\n",
       "      <td>...</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "      <td>35.600070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>17.216478</td>\n",
       "      <td>...</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "      <td>34.967198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>17.261567</td>\n",
       "      <td>...</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "      <td>35.251771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>17.799714</td>\n",
       "      <td>...</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "      <td>38.438689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>17.346859</td>\n",
       "      <td>...</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "      <td>35.400399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>17.680272</td>\n",
       "      <td>...</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "      <td>38.140113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>18.127111</td>\n",
       "      <td>...</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "      <td>38.867769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25.341497</td>\n",
       "      <td>23.531973</td>\n",
       "      <td>21.797279</td>\n",
       "      <td>20.092517</td>\n",
       "      <td>18.421769</td>\n",
       "      <td>17.406803</td>\n",
       "      <td>16.936728</td>\n",
       "      <td>15.961905</td>\n",
       "      <td>14.601361</td>\n",
       "      <td>12.983673</td>\n",
       "      <td>...</td>\n",
       "      <td>44.357823</td>\n",
       "      <td>43.225850</td>\n",
       "      <td>42.359184</td>\n",
       "      <td>40.846259</td>\n",
       "      <td>38.395918</td>\n",
       "      <td>36.707483</td>\n",
       "      <td>33.469388</td>\n",
       "      <td>21.982313</td>\n",
       "      <td>6.468027</td>\n",
       "      <td>5.702041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26.968707</td>\n",
       "      <td>25.206803</td>\n",
       "      <td>23.404082</td>\n",
       "      <td>21.502041</td>\n",
       "      <td>19.823129</td>\n",
       "      <td>18.614966</td>\n",
       "      <td>18.244898</td>\n",
       "      <td>17.136054</td>\n",
       "      <td>15.745578</td>\n",
       "      <td>13.993197</td>\n",
       "      <td>...</td>\n",
       "      <td>47.835374</td>\n",
       "      <td>46.511565</td>\n",
       "      <td>45.653061</td>\n",
       "      <td>44.038095</td>\n",
       "      <td>41.570068</td>\n",
       "      <td>39.712925</td>\n",
       "      <td>36.478912</td>\n",
       "      <td>24.157823</td>\n",
       "      <td>7.658503</td>\n",
       "      <td>6.825850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23.055782</td>\n",
       "      <td>21.994558</td>\n",
       "      <td>20.797279</td>\n",
       "      <td>19.537415</td>\n",
       "      <td>18.379592</td>\n",
       "      <td>17.567347</td>\n",
       "      <td>17.677551</td>\n",
       "      <td>17.378231</td>\n",
       "      <td>16.952381</td>\n",
       "      <td>15.926531</td>\n",
       "      <td>...</td>\n",
       "      <td>38.964626</td>\n",
       "      <td>38.289796</td>\n",
       "      <td>38.311565</td>\n",
       "      <td>37.714286</td>\n",
       "      <td>35.676190</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>31.717007</td>\n",
       "      <td>21.776871</td>\n",
       "      <td>7.672109</td>\n",
       "      <td>6.748299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.639456</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.208163</td>\n",
       "      <td>19.953741</td>\n",
       "      <td>18.775510</td>\n",
       "      <td>18.008163</td>\n",
       "      <td>18.076190</td>\n",
       "      <td>17.787755</td>\n",
       "      <td>17.296599</td>\n",
       "      <td>16.325170</td>\n",
       "      <td>...</td>\n",
       "      <td>40.172789</td>\n",
       "      <td>39.425850</td>\n",
       "      <td>39.478912</td>\n",
       "      <td>38.790476</td>\n",
       "      <td>36.620408</td>\n",
       "      <td>35.053061</td>\n",
       "      <td>32.553741</td>\n",
       "      <td>22.152381</td>\n",
       "      <td>7.926531</td>\n",
       "      <td>6.938776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>22.740136</td>\n",
       "      <td>21.610884</td>\n",
       "      <td>20.442177</td>\n",
       "      <td>19.206803</td>\n",
       "      <td>18.013605</td>\n",
       "      <td>17.285714</td>\n",
       "      <td>17.443537</td>\n",
       "      <td>17.161905</td>\n",
       "      <td>16.725170</td>\n",
       "      <td>15.746939</td>\n",
       "      <td>...</td>\n",
       "      <td>38.687075</td>\n",
       "      <td>37.974150</td>\n",
       "      <td>38.112925</td>\n",
       "      <td>37.317007</td>\n",
       "      <td>35.363265</td>\n",
       "      <td>33.802721</td>\n",
       "      <td>31.421769</td>\n",
       "      <td>21.410884</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>6.717007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26.885714</td>\n",
       "      <td>25.025850</td>\n",
       "      <td>23.257143</td>\n",
       "      <td>21.571429</td>\n",
       "      <td>19.989116</td>\n",
       "      <td>18.816327</td>\n",
       "      <td>18.368707</td>\n",
       "      <td>17.321088</td>\n",
       "      <td>15.919728</td>\n",
       "      <td>14.123810</td>\n",
       "      <td>...</td>\n",
       "      <td>46.628571</td>\n",
       "      <td>45.560544</td>\n",
       "      <td>44.590476</td>\n",
       "      <td>42.967347</td>\n",
       "      <td>40.423129</td>\n",
       "      <td>38.580952</td>\n",
       "      <td>35.375510</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>6.402721</td>\n",
       "      <td>5.560544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26.553741</td>\n",
       "      <td>24.657143</td>\n",
       "      <td>22.868027</td>\n",
       "      <td>21.076190</td>\n",
       "      <td>19.401361</td>\n",
       "      <td>18.197279</td>\n",
       "      <td>17.825850</td>\n",
       "      <td>16.742857</td>\n",
       "      <td>15.378231</td>\n",
       "      <td>13.620408</td>\n",
       "      <td>...</td>\n",
       "      <td>46.946939</td>\n",
       "      <td>45.604082</td>\n",
       "      <td>44.692517</td>\n",
       "      <td>43.110204</td>\n",
       "      <td>40.497959</td>\n",
       "      <td>38.734557</td>\n",
       "      <td>35.371429</td>\n",
       "      <td>23.401361</td>\n",
       "      <td>7.118367</td>\n",
       "      <td>6.296599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22.427211</td>\n",
       "      <td>21.193197</td>\n",
       "      <td>20.039456</td>\n",
       "      <td>18.765986</td>\n",
       "      <td>17.681633</td>\n",
       "      <td>16.929252</td>\n",
       "      <td>16.963265</td>\n",
       "      <td>16.780952</td>\n",
       "      <td>16.315646</td>\n",
       "      <td>15.413605</td>\n",
       "      <td>...</td>\n",
       "      <td>38.351020</td>\n",
       "      <td>37.687075</td>\n",
       "      <td>37.642177</td>\n",
       "      <td>36.949660</td>\n",
       "      <td>34.912925</td>\n",
       "      <td>33.336054</td>\n",
       "      <td>31.043537</td>\n",
       "      <td>21.114286</td>\n",
       "      <td>7.760544</td>\n",
       "      <td>6.805442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>26.444898</td>\n",
       "      <td>24.634014</td>\n",
       "      <td>22.876190</td>\n",
       "      <td>21.085714</td>\n",
       "      <td>19.461224</td>\n",
       "      <td>18.326531</td>\n",
       "      <td>17.915646</td>\n",
       "      <td>16.866667</td>\n",
       "      <td>15.541497</td>\n",
       "      <td>13.786395</td>\n",
       "      <td>...</td>\n",
       "      <td>45.980952</td>\n",
       "      <td>44.779592</td>\n",
       "      <td>44.084354</td>\n",
       "      <td>42.500680</td>\n",
       "      <td>39.813605</td>\n",
       "      <td>38.152381</td>\n",
       "      <td>35.178231</td>\n",
       "      <td>22.476190</td>\n",
       "      <td>6.949660</td>\n",
       "      <td>6.069388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25.571429</td>\n",
       "      <td>23.828571</td>\n",
       "      <td>22.189116</td>\n",
       "      <td>20.408163</td>\n",
       "      <td>18.786395</td>\n",
       "      <td>17.778231</td>\n",
       "      <td>17.368707</td>\n",
       "      <td>16.367347</td>\n",
       "      <td>15.040816</td>\n",
       "      <td>13.331973</td>\n",
       "      <td>...</td>\n",
       "      <td>44.272109</td>\n",
       "      <td>43.145578</td>\n",
       "      <td>42.439456</td>\n",
       "      <td>40.960544</td>\n",
       "      <td>38.482993</td>\n",
       "      <td>36.774150</td>\n",
       "      <td>33.546939</td>\n",
       "      <td>21.897959</td>\n",
       "      <td>6.336054</td>\n",
       "      <td>5.606803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>26.088435</td>\n",
       "      <td>24.277551</td>\n",
       "      <td>22.595918</td>\n",
       "      <td>20.794558</td>\n",
       "      <td>19.208163</td>\n",
       "      <td>18.131973</td>\n",
       "      <td>17.745578</td>\n",
       "      <td>16.704762</td>\n",
       "      <td>15.416327</td>\n",
       "      <td>13.681633</td>\n",
       "      <td>...</td>\n",
       "      <td>45.774150</td>\n",
       "      <td>44.474830</td>\n",
       "      <td>43.741497</td>\n",
       "      <td>42.145578</td>\n",
       "      <td>39.434014</td>\n",
       "      <td>37.485714</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>22.127891</td>\n",
       "      <td>5.922449</td>\n",
       "      <td>5.194558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>22.616327</td>\n",
       "      <td>21.551020</td>\n",
       "      <td>20.451701</td>\n",
       "      <td>19.082993</td>\n",
       "      <td>18.013605</td>\n",
       "      <td>17.349660</td>\n",
       "      <td>17.304762</td>\n",
       "      <td>17.095238</td>\n",
       "      <td>16.614966</td>\n",
       "      <td>15.711565</td>\n",
       "      <td>...</td>\n",
       "      <td>38.261224</td>\n",
       "      <td>37.413605</td>\n",
       "      <td>37.608163</td>\n",
       "      <td>36.942857</td>\n",
       "      <td>34.933333</td>\n",
       "      <td>33.412245</td>\n",
       "      <td>31.248980</td>\n",
       "      <td>20.971429</td>\n",
       "      <td>7.627211</td>\n",
       "      <td>6.627211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 103680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5  \\\n",
       "0   25.612245  23.771429  18.341497   3.925170   4.761905  15.587755   \n",
       "1   25.749660  23.971429  18.451701   3.868027   4.624490  15.868027   \n",
       "2   25.725170  24.070748  18.303401   3.914286   4.012245  16.587755   \n",
       "3   26.780952  25.200000  19.444898   3.891156   4.248980  17.557823   \n",
       "4   26.457143  24.828571  19.061224   3.869388   4.195918  17.287075   \n",
       "5   25.548299  23.785034  18.357823   3.948299   5.053061  15.444898   \n",
       "6   26.268027  24.627211  18.546939   3.838095   4.099320  16.644898   \n",
       "7   24.129252  22.546939  17.331973   3.643537   4.127891  14.978231   \n",
       "8   25.778231  23.959184  18.512925   3.956463   4.779592  15.725170   \n",
       "9   25.840816  24.195918  18.447619   3.881633   4.093878  16.447619   \n",
       "10  26.534694  24.891156  19.244898   3.831293   4.400000  17.142857   \n",
       "11  17.427362  17.427362  17.427362  17.427362  17.427362  17.427362   \n",
       "12  18.158054  18.158054  18.158054  18.158054  18.158054  18.158054   \n",
       "13  18.328894  18.328894  18.328894  18.328894  18.328894  18.328894   \n",
       "14  18.101834  18.101834  18.101834  18.101834  18.101834  18.101834   \n",
       "15  17.846282  17.846282  17.846282  17.846282  17.846282  17.846282   \n",
       "16  17.491713  17.491713  17.491713  17.491713  17.491713  17.491713   \n",
       "17  17.216478  17.216478  17.216478  17.216478  17.216478  17.216478   \n",
       "18  17.261567  17.261567  17.261567  17.261567  17.261567  17.261567   \n",
       "19  17.799714  17.799714  17.799714  17.799714  17.799714  17.799714   \n",
       "20  17.346859  17.346859  17.346859  17.346859  17.346859  17.346859   \n",
       "21  17.680272  17.680272  17.680272  17.680272  17.680272  17.680272   \n",
       "22  18.127111  18.127111  18.127111  18.127111  18.127111  18.127111   \n",
       "23  25.341497  23.531973  21.797279  20.092517  18.421769  17.406803   \n",
       "24  26.968707  25.206803  23.404082  21.502041  19.823129  18.614966   \n",
       "25  23.055782  21.994558  20.797279  19.537415  18.379592  17.567347   \n",
       "26  23.639456  22.400000  21.208163  19.953741  18.775510  18.008163   \n",
       "27  22.740136  21.610884  20.442177  19.206803  18.013605  17.285714   \n",
       "28  26.885714  25.025850  23.257143  21.571429  19.989116  18.816327   \n",
       "29  26.553741  24.657143  22.868027  21.076190  19.401361  18.197279   \n",
       "30  22.427211  21.193197  20.039456  18.765986  17.681633  16.929252   \n",
       "31  26.444898  24.634014  22.876190  21.085714  19.461224  18.326531   \n",
       "32  25.571429  23.828571  22.189116  20.408163  18.786395  17.778231   \n",
       "33  26.088435  24.277551  22.595918  20.794558  19.208163  18.131973   \n",
       "34  22.616327  21.551020  20.451701  19.082993  18.013605  17.349660   \n",
       "\n",
       "            6          7          8          9  ...     103670     103671  \\\n",
       "0   16.447619  10.442177  12.428571   6.717007  ...  43.736054  42.488435   \n",
       "1   16.703401  10.451701  12.578231   6.692517  ...  44.696599  43.454422   \n",
       "2   16.337415  10.853061  12.319728   6.722449  ...  44.721088  43.390476   \n",
       "3   18.242177  10.834014  13.568707   6.640816  ...  43.934694  42.791837   \n",
       "4   17.944218  10.682993  13.353741   6.696599  ...  43.980952  42.767347   \n",
       "5   16.398639  10.702041  12.380952   6.741497  ...  44.436735  42.955102   \n",
       "6   16.937415  10.206803  12.495238   6.786395  ...  42.507483  35.410884   \n",
       "7   15.704762   9.718367  11.963265   6.187755  ...  42.770068  41.612245   \n",
       "8   16.526531  10.544218  12.523810   6.729252  ...  44.179592  42.805442   \n",
       "9   16.834014  10.232653  12.440816   6.797279  ...  44.839456  43.468027   \n",
       "10  17.991837  10.800000  13.492517   6.553741  ...  44.080272  42.866667   \n",
       "11  17.427362  17.427362  17.427362  17.427362  ...  35.471421  35.471421   \n",
       "12  18.158054  18.158054  18.158054  18.158054  ...  38.748870  38.748870   \n",
       "13  18.328894  18.328894  18.328894  18.328894  ...  39.104379  39.104379   \n",
       "14  18.101834  18.101834  18.101834  18.101834  ...  38.542849  38.542849   \n",
       "15  17.846282  17.846282  17.846282  17.846282  ...  38.131652  38.131652   \n",
       "16  17.491713  17.491713  17.491713  17.491713  ...  35.600070  35.600070   \n",
       "17  17.216478  17.216478  17.216478  17.216478  ...  34.967198  34.967198   \n",
       "18  17.261567  17.261567  17.261567  17.261567  ...  35.251771  35.251771   \n",
       "19  17.799714  17.799714  17.799714  17.799714  ...  38.438689  38.438689   \n",
       "20  17.346859  17.346859  17.346859  17.346859  ...  35.400399  35.400399   \n",
       "21  17.680272  17.680272  17.680272  17.680272  ...  38.140113  38.140113   \n",
       "22  18.127111  18.127111  18.127111  18.127111  ...  38.867769  38.867769   \n",
       "23  16.936728  15.961905  14.601361  12.983673  ...  44.357823  43.225850   \n",
       "24  18.244898  17.136054  15.745578  13.993197  ...  47.835374  46.511565   \n",
       "25  17.677551  17.378231  16.952381  15.926531  ...  38.964626  38.289796   \n",
       "26  18.076190  17.787755  17.296599  16.325170  ...  40.172789  39.425850   \n",
       "27  17.443537  17.161905  16.725170  15.746939  ...  38.687075  37.974150   \n",
       "28  18.368707  17.321088  15.919728  14.123810  ...  46.628571  45.560544   \n",
       "29  17.825850  16.742857  15.378231  13.620408  ...  46.946939  45.604082   \n",
       "30  16.963265  16.780952  16.315646  15.413605  ...  38.351020  37.687075   \n",
       "31  17.915646  16.866667  15.541497  13.786395  ...  45.980952  44.779592   \n",
       "32  17.368707  16.367347  15.040816  13.331973  ...  44.272109  43.145578   \n",
       "33  17.745578  16.704762  15.416327  13.681633  ...  45.774150  44.474830   \n",
       "34  17.304762  17.095238  16.614966  15.711565  ...  38.261224  37.413605   \n",
       "\n",
       "       103672     103673     103674     103675     103676     103677  \\\n",
       "0   41.051701  39.389116  37.186395  35.801361  33.794558  18.436735   \n",
       "1   41.942857  40.287075  38.038095  36.454422  34.304762  18.800000   \n",
       "2   41.944218  40.225850  38.035374  36.477551  34.224490  18.157823   \n",
       "3   41.670748  39.816327  37.857143  36.507483  34.556463  17.353741   \n",
       "4   41.289796  39.642177  37.447619  36.341497  34.293878  17.238095   \n",
       "5   41.545578  39.817687  37.710204  36.108844  33.960544  18.186395   \n",
       "6   26.703401  19.514286  13.745578  11.927891  10.303401   7.288435   \n",
       "7   40.208163  38.642177  36.843537  35.292517  33.368707  16.865306   \n",
       "8   41.390476  39.629932  37.440816  36.023129  34.048980  18.526531   \n",
       "9   41.910204  40.216327  37.910204  36.499320  34.387755  17.778231   \n",
       "10  41.466667  39.899320  37.907483  36.542857  34.610884  17.136054   \n",
       "11  35.471421  35.471421  35.471421  35.471421  35.471421  35.471421   \n",
       "12  38.748870  38.748870  38.748870  38.748870  38.748870  38.748870   \n",
       "13  39.104379  39.104379  39.104379  39.104379  39.104379  39.104379   \n",
       "14  38.542849  38.542849  38.542849  38.542849  38.542849  38.542849   \n",
       "15  38.131652  38.131652  38.131652  38.131652  38.131652  38.131652   \n",
       "16  35.600070  35.600070  35.600070  35.600070  35.600070  35.600070   \n",
       "17  34.967198  34.967198  34.967198  34.967198  34.967198  34.967198   \n",
       "18  35.251771  35.251771  35.251771  35.251771  35.251771  35.251771   \n",
       "19  38.438689  38.438689  38.438689  38.438689  38.438689  38.438689   \n",
       "20  35.400399  35.400399  35.400399  35.400399  35.400399  35.400399   \n",
       "21  38.140113  38.140113  38.140113  38.140113  38.140113  38.140113   \n",
       "22  38.867769  38.867769  38.867769  38.867769  38.867769  38.867769   \n",
       "23  42.359184  40.846259  38.395918  36.707483  33.469388  21.982313   \n",
       "24  45.653061  44.038095  41.570068  39.712925  36.478912  24.157823   \n",
       "25  38.311565  37.714286  35.676190  34.114286  31.717007  21.776871   \n",
       "26  39.478912  38.790476  36.620408  35.053061  32.553741  22.152381   \n",
       "27  38.112925  37.317007  35.363265  33.802721  31.421769  21.410884   \n",
       "28  44.590476  42.967347  40.423129  38.580952  35.375510  22.733333   \n",
       "29  44.692517  43.110204  40.497959  38.734557  35.371429  23.401361   \n",
       "30  37.642177  36.949660  34.912925  33.336054  31.043537  21.114286   \n",
       "31  44.084354  42.500680  39.813605  38.152381  35.178231  22.476190   \n",
       "32  42.439456  40.960544  38.482993  36.774150  33.546939  21.897959   \n",
       "33  43.741497  42.145578  39.434014  37.485714  34.114286  22.127891   \n",
       "34  37.608163  36.942857  34.933333  33.412245  31.248980  20.971429   \n",
       "\n",
       "       103678     103679  \n",
       "0   28.198639  14.804082  \n",
       "1   28.595918  15.008163  \n",
       "2   29.416327  15.093878  \n",
       "3   30.627211  15.142857  \n",
       "4   30.289796  15.117007  \n",
       "5   28.808163  14.918367  \n",
       "6    9.053061   7.529252  \n",
       "7   29.711565  14.817687  \n",
       "8   28.393197  14.982313  \n",
       "9   29.835374  15.106122  \n",
       "10  30.808163  15.153741  \n",
       "11  35.471421  35.471421  \n",
       "12  38.748870  38.748870  \n",
       "13  39.104379  39.104379  \n",
       "14  38.542849  38.542849  \n",
       "15  38.131652  38.131652  \n",
       "16  35.600070  35.600070  \n",
       "17  34.967198  34.967198  \n",
       "18  35.251771  35.251771  \n",
       "19  38.438689  38.438689  \n",
       "20  35.400399  35.400399  \n",
       "21  38.140113  38.140113  \n",
       "22  38.867769  38.867769  \n",
       "23   6.468027   5.702041  \n",
       "24   7.658503   6.825850  \n",
       "25   7.672109   6.748299  \n",
       "26   7.926531   6.938776  \n",
       "27   7.600000   6.717007  \n",
       "28   6.402721   5.560544  \n",
       "29   7.118367   6.296599  \n",
       "30   7.760544   6.805442  \n",
       "31   6.949660   6.069388  \n",
       "32   6.336054   5.606803  \n",
       "33   5.922449   5.194558  \n",
       "34   7.627211   6.627211  \n",
       "\n",
       "[35 rows x 103680 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# please change the path according to your setting\n",
    "W = pd.read_csv('W_35.csv')\n",
    "D_O = pd.read_csv('norm_power_35.csv')\n",
    "D_O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o03gr5pipODF",
   "metadata": {
    "id": "o03gr5pipODF"
   },
   "source": [
    "#Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf38ec8-c553-48e2-81e4-4b5ff7a42df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "1bf38ec8-c553-48e2-81e4-4b5ff7a42df7",
    "outputId": "0400a649-65ee-408f-a63b-c2667274dd09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.612245</td>\n",
       "      <td>25.749660</td>\n",
       "      <td>25.725170</td>\n",
       "      <td>26.780952</td>\n",
       "      <td>26.457143</td>\n",
       "      <td>25.548299</td>\n",
       "      <td>26.268027</td>\n",
       "      <td>24.129252</td>\n",
       "      <td>25.778231</td>\n",
       "      <td>25.840816</td>\n",
       "      <td>...</td>\n",
       "      <td>23.055782</td>\n",
       "      <td>23.639456</td>\n",
       "      <td>22.740136</td>\n",
       "      <td>26.885714</td>\n",
       "      <td>26.553741</td>\n",
       "      <td>22.427211</td>\n",
       "      <td>26.444898</td>\n",
       "      <td>25.571429</td>\n",
       "      <td>26.088435</td>\n",
       "      <td>22.616327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.771429</td>\n",
       "      <td>23.971429</td>\n",
       "      <td>24.070748</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>24.828571</td>\n",
       "      <td>23.785034</td>\n",
       "      <td>24.627211</td>\n",
       "      <td>22.546939</td>\n",
       "      <td>23.959184</td>\n",
       "      <td>24.195918</td>\n",
       "      <td>...</td>\n",
       "      <td>21.994558</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.610884</td>\n",
       "      <td>25.025850</td>\n",
       "      <td>24.657143</td>\n",
       "      <td>21.193197</td>\n",
       "      <td>24.634014</td>\n",
       "      <td>23.828571</td>\n",
       "      <td>24.277551</td>\n",
       "      <td>21.551020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.341497</td>\n",
       "      <td>18.451701</td>\n",
       "      <td>18.303401</td>\n",
       "      <td>19.444898</td>\n",
       "      <td>19.061224</td>\n",
       "      <td>18.357823</td>\n",
       "      <td>18.546939</td>\n",
       "      <td>17.331973</td>\n",
       "      <td>18.512925</td>\n",
       "      <td>18.447619</td>\n",
       "      <td>...</td>\n",
       "      <td>20.797279</td>\n",
       "      <td>21.208163</td>\n",
       "      <td>20.442177</td>\n",
       "      <td>23.257143</td>\n",
       "      <td>22.868027</td>\n",
       "      <td>20.039456</td>\n",
       "      <td>22.876190</td>\n",
       "      <td>22.189116</td>\n",
       "      <td>22.595918</td>\n",
       "      <td>20.451701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.925170</td>\n",
       "      <td>3.868027</td>\n",
       "      <td>3.914286</td>\n",
       "      <td>3.891156</td>\n",
       "      <td>3.869388</td>\n",
       "      <td>3.948299</td>\n",
       "      <td>3.838095</td>\n",
       "      <td>3.643537</td>\n",
       "      <td>3.956463</td>\n",
       "      <td>3.881633</td>\n",
       "      <td>...</td>\n",
       "      <td>19.537415</td>\n",
       "      <td>19.953741</td>\n",
       "      <td>19.206803</td>\n",
       "      <td>21.571429</td>\n",
       "      <td>21.076190</td>\n",
       "      <td>18.765986</td>\n",
       "      <td>21.085714</td>\n",
       "      <td>20.408163</td>\n",
       "      <td>20.794558</td>\n",
       "      <td>19.082993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.624490</td>\n",
       "      <td>4.012245</td>\n",
       "      <td>4.248980</td>\n",
       "      <td>4.195918</td>\n",
       "      <td>5.053061</td>\n",
       "      <td>4.099320</td>\n",
       "      <td>4.127891</td>\n",
       "      <td>4.779592</td>\n",
       "      <td>4.093878</td>\n",
       "      <td>...</td>\n",
       "      <td>18.379592</td>\n",
       "      <td>18.775510</td>\n",
       "      <td>18.013605</td>\n",
       "      <td>19.989116</td>\n",
       "      <td>19.401361</td>\n",
       "      <td>17.681633</td>\n",
       "      <td>19.461224</td>\n",
       "      <td>18.786395</td>\n",
       "      <td>19.208163</td>\n",
       "      <td>18.013605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103675</th>\n",
       "      <td>35.801361</td>\n",
       "      <td>36.454422</td>\n",
       "      <td>36.477551</td>\n",
       "      <td>36.507483</td>\n",
       "      <td>36.341497</td>\n",
       "      <td>36.108844</td>\n",
       "      <td>11.927891</td>\n",
       "      <td>35.292517</td>\n",
       "      <td>36.023129</td>\n",
       "      <td>36.499320</td>\n",
       "      <td>...</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>35.053061</td>\n",
       "      <td>33.802721</td>\n",
       "      <td>38.580952</td>\n",
       "      <td>38.734557</td>\n",
       "      <td>33.336054</td>\n",
       "      <td>38.152381</td>\n",
       "      <td>36.774150</td>\n",
       "      <td>37.485714</td>\n",
       "      <td>33.412245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103676</th>\n",
       "      <td>33.794558</td>\n",
       "      <td>34.304762</td>\n",
       "      <td>34.224490</td>\n",
       "      <td>34.556463</td>\n",
       "      <td>34.293878</td>\n",
       "      <td>33.960544</td>\n",
       "      <td>10.303401</td>\n",
       "      <td>33.368707</td>\n",
       "      <td>34.048980</td>\n",
       "      <td>34.387755</td>\n",
       "      <td>...</td>\n",
       "      <td>31.717007</td>\n",
       "      <td>32.553741</td>\n",
       "      <td>31.421769</td>\n",
       "      <td>35.375510</td>\n",
       "      <td>35.371429</td>\n",
       "      <td>31.043537</td>\n",
       "      <td>35.178231</td>\n",
       "      <td>33.546939</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>31.248980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103677</th>\n",
       "      <td>18.436735</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>18.157823</td>\n",
       "      <td>17.353741</td>\n",
       "      <td>17.238095</td>\n",
       "      <td>18.186395</td>\n",
       "      <td>7.288435</td>\n",
       "      <td>16.865306</td>\n",
       "      <td>18.526531</td>\n",
       "      <td>17.778231</td>\n",
       "      <td>...</td>\n",
       "      <td>21.776871</td>\n",
       "      <td>22.152381</td>\n",
       "      <td>21.410884</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>23.401361</td>\n",
       "      <td>21.114286</td>\n",
       "      <td>22.476190</td>\n",
       "      <td>21.897959</td>\n",
       "      <td>22.127891</td>\n",
       "      <td>20.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103678</th>\n",
       "      <td>28.198639</td>\n",
       "      <td>28.595918</td>\n",
       "      <td>29.416327</td>\n",
       "      <td>30.627211</td>\n",
       "      <td>30.289796</td>\n",
       "      <td>28.808163</td>\n",
       "      <td>9.053061</td>\n",
       "      <td>29.711565</td>\n",
       "      <td>28.393197</td>\n",
       "      <td>29.835374</td>\n",
       "      <td>...</td>\n",
       "      <td>7.672109</td>\n",
       "      <td>7.926531</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>6.402721</td>\n",
       "      <td>7.118367</td>\n",
       "      <td>7.760544</td>\n",
       "      <td>6.949660</td>\n",
       "      <td>6.336054</td>\n",
       "      <td>5.922449</td>\n",
       "      <td>7.627211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103679</th>\n",
       "      <td>14.804082</td>\n",
       "      <td>15.008163</td>\n",
       "      <td>15.093878</td>\n",
       "      <td>15.142857</td>\n",
       "      <td>15.117007</td>\n",
       "      <td>14.918367</td>\n",
       "      <td>7.529252</td>\n",
       "      <td>14.817687</td>\n",
       "      <td>14.982313</td>\n",
       "      <td>15.106122</td>\n",
       "      <td>...</td>\n",
       "      <td>6.748299</td>\n",
       "      <td>6.938776</td>\n",
       "      <td>6.717007</td>\n",
       "      <td>5.560544</td>\n",
       "      <td>6.296599</td>\n",
       "      <td>6.805442</td>\n",
       "      <td>6.069388</td>\n",
       "      <td>5.606803</td>\n",
       "      <td>5.194558</td>\n",
       "      <td>6.627211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103680 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5   \\\n",
       "0       25.612245  25.749660  25.725170  26.780952  26.457143  25.548299   \n",
       "1       23.771429  23.971429  24.070748  25.200000  24.828571  23.785034   \n",
       "2       18.341497  18.451701  18.303401  19.444898  19.061224  18.357823   \n",
       "3        3.925170   3.868027   3.914286   3.891156   3.869388   3.948299   \n",
       "4        4.761905   4.624490   4.012245   4.248980   4.195918   5.053061   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "103675  35.801361  36.454422  36.477551  36.507483  36.341497  36.108844   \n",
       "103676  33.794558  34.304762  34.224490  34.556463  34.293878  33.960544   \n",
       "103677  18.436735  18.800000  18.157823  17.353741  17.238095  18.186395   \n",
       "103678  28.198639  28.595918  29.416327  30.627211  30.289796  28.808163   \n",
       "103679  14.804082  15.008163  15.093878  15.142857  15.117007  14.918367   \n",
       "\n",
       "               6          7          8          9   ...         25         26  \\\n",
       "0       26.268027  24.129252  25.778231  25.840816  ...  23.055782  23.639456   \n",
       "1       24.627211  22.546939  23.959184  24.195918  ...  21.994558  22.400000   \n",
       "2       18.546939  17.331973  18.512925  18.447619  ...  20.797279  21.208163   \n",
       "3        3.838095   3.643537   3.956463   3.881633  ...  19.537415  19.953741   \n",
       "4        4.099320   4.127891   4.779592   4.093878  ...  18.379592  18.775510   \n",
       "...           ...        ...        ...        ...  ...        ...        ...   \n",
       "103675  11.927891  35.292517  36.023129  36.499320  ...  34.114286  35.053061   \n",
       "103676  10.303401  33.368707  34.048980  34.387755  ...  31.717007  32.553741   \n",
       "103677   7.288435  16.865306  18.526531  17.778231  ...  21.776871  22.152381   \n",
       "103678   9.053061  29.711565  28.393197  29.835374  ...   7.672109   7.926531   \n",
       "103679   7.529252  14.817687  14.982313  15.106122  ...   6.748299   6.938776   \n",
       "\n",
       "               27         28         29         30         31         32  \\\n",
       "0       22.740136  26.885714  26.553741  22.427211  26.444898  25.571429   \n",
       "1       21.610884  25.025850  24.657143  21.193197  24.634014  23.828571   \n",
       "2       20.442177  23.257143  22.868027  20.039456  22.876190  22.189116   \n",
       "3       19.206803  21.571429  21.076190  18.765986  21.085714  20.408163   \n",
       "4       18.013605  19.989116  19.401361  17.681633  19.461224  18.786395   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "103675  33.802721  38.580952  38.734557  33.336054  38.152381  36.774150   \n",
       "103676  31.421769  35.375510  35.371429  31.043537  35.178231  33.546939   \n",
       "103677  21.410884  22.733333  23.401361  21.114286  22.476190  21.897959   \n",
       "103678   7.600000   6.402721   7.118367   7.760544   6.949660   6.336054   \n",
       "103679   6.717007   5.560544   6.296599   6.805442   6.069388   5.606803   \n",
       "\n",
       "               33         34  \n",
       "0       26.088435  22.616327  \n",
       "1       24.277551  21.551020  \n",
       "2       22.595918  20.451701  \n",
       "3       20.794558  19.082993  \n",
       "4       19.208163  18.013605  \n",
       "...           ...        ...  \n",
       "103675  37.485714  33.412245  \n",
       "103676  34.114286  31.248980  \n",
       "103677  22.127891  20.971429  \n",
       "103678   5.922449   7.627211  \n",
       "103679   5.194558   6.627211  \n",
       "\n",
       "[103680 rows x 35 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "D_O = D_O.T\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "D_O = pd.DataFrame(D_O)\n",
    "imputer.fit(D_O)\n",
    "D_A = imputer.transform(D_O)\n",
    "D_A = pd.DataFrame(D_A)\n",
    "D_A.to_csv('Data_Augmented.csv', index = False) \n",
    "D_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3tOPayWnp-Zf",
   "metadata": {
    "id": "3tOPayWnp-Zf"
   },
   "source": [
    "#Data Corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PUkEbxPPqOw1",
   "metadata": {
    "id": "PUkEbxPPqOw1"
   },
   "source": [
    "We provide the choice of 12 different missing masks to corrupt, which includes:\n",
    "\n",
    "\n",
    "1.   Type I: Missing Completely at Random (MCAR): 10%, 20%, 30%, 40%, 50%, and 60%\n",
    "2.   Type II: Block Missing (BM): 2hrs, 4hrs, 6hrs, 8hrs, 10hrs, and 12hrs.\n",
    "\n",
    "The missing maks can be adjusted if the prior distribution of missing data is given.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5ebdcJwqO7h",
   "metadata": {
    "id": "o5ebdcJwqO7h"
   },
   "source": [
    "MCAR Masks\n",
    "\n",
    "Please change the path according to your setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YUb1wETurXSU",
   "metadata": {
    "id": "YUb1wETurXSU"
   },
   "outputs": [],
   "source": [
    "mask = torch.FloatTensor(103680, 98).uniform_() > 0.2\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/20%MCAR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hptXD-6MrXU9",
   "metadata": {
    "id": "hptXD-6MrXU9"
   },
   "outputs": [],
   "source": [
    "mask = torch.FloatTensor(103680, 98).uniform_() > 0.3\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/30%MCAR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RRyzvNv9reBg",
   "metadata": {
    "id": "RRyzvNv9reBg"
   },
   "outputs": [],
   "source": [
    "mask = torch.FloatTensor(103680, 98).uniform_() > 0.4\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/40%MCAR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RPYYz1yMreFM",
   "metadata": {
    "id": "RPYYz1yMreFM"
   },
   "outputs": [],
   "source": [
    "mask = torch.FloatTensor(103680, 98).uniform_() > 0.5\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/50%MCAR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_eTHVZzYreII",
   "metadata": {
    "id": "_eTHVZzYreII"
   },
   "outputs": [],
   "source": [
    "mask = torch.FloatTensor(103680, 98).uniform_() > 0.6\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/60%MCAR.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X4RXqWl3rkBJ",
   "metadata": {
    "id": "X4RXqWl3rkBJ"
   },
   "source": [
    "BM Masks\n",
    "Please change the path according to your setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4hixu8areK6",
   "metadata": {
    "id": "a4hixu8areK6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=24\n",
    "mask = torch.full((103680,98), True)\n",
    "for i in range(360):\n",
    "    for j in range(98):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/2hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uGoEiECsrnQL",
   "metadata": {
    "id": "uGoEiECsrnQL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=48\n",
    "mask = torch.full((103680,35), True)\n",
    "for i in range(360):\n",
    "    for j in range(35):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('4hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "MeT_iUQbrnUQ",
   "metadata": {
    "id": "MeT_iUQbrnUQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=72\n",
    "mask = torch.full((103680,35), True)\n",
    "for i in range(360):\n",
    "    for j in range(35):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('6hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LJ2DVVtRrnXv",
   "metadata": {
    "id": "LJ2DVVtRrnXv"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=96\n",
    "mask = torch.full((103680,98), True)\n",
    "for i in range(360):\n",
    "    for j in range(98):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/8hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809T0mMxrndr",
   "metadata": {
    "id": "809T0mMxrndr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=120\n",
    "mask = torch.full((103680,98), True)\n",
    "for i in range(360):\n",
    "    for j in range(98):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('gdrive/My Drive/STGCN/10hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aIokuexcrnmq",
   "metadata": {
    "id": "aIokuexcrnmq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length=144\n",
    "mask = torch.full((103680,35), True)\n",
    "for i in range(360):\n",
    "    for j in range(35):\n",
    "        number = random.randint(288*i+1,288*(i+1)-length-1)\n",
    "        mask[number:number+length, j] = False\n",
    "\n",
    "pd.DataFrame(mask.numpy()).to_csv('12hr_BM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a54b8116-cea3-4758-bf2b-4e46f58b474b",
   "metadata": {
    "id": "a54b8116-cea3-4758-bf2b-4e46f58b474b"
   },
   "outputs": [],
   "source": [
    "#Choose the mask you want to corrupt D_A: here we choose 12hrs BM\n",
    "mask = pd.read_csv('12hr_BM.csv')\n",
    "mask = torch.tensor(mask.values)\n",
    "D_C = pd.read_csv('Data_Augmented.csv')\n",
    "D_C[mask.numpy()==False] = -1\n",
    "D_A = pd.read_csv('Data_Augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EcZw-IpCtJad",
   "metadata": {
    "id": "EcZw-IpCtJad"
   },
   "source": [
    "#STD-GAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ek7azuCztLk2",
   "metadata": {
    "id": "ek7azuCztLk2"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e49ec5-c1fc-4a7e-be49-e8f25217c24c",
   "metadata": {
    "id": "f4e49ec5-c1fc-4a7e-be49-e8f25217c24c"
   },
   "outputs": [],
   "source": [
    "power_tensor = torch.tensor(D_A.values)\n",
    "length = D_A.shape[0]\n",
    "train_x = power_tensor[0:int(train_prop*length),:].to(torch.float32)\n",
    "validation_x = power_tensor[int(train_prop*length):int((train_prop+val_prop)*length)+1,:].to(torch.float32)\n",
    "test_x = power_tensor[int((train_prop+val_prop)*length)+1:length,:].to(torch.float32)\n",
    "\n",
    "power_corrupted_tensor = torch.tensor(D_C.values)\n",
    "length = D_C.shape[0]\n",
    "corrupted_train_x = power_corrupted_tensor[0:int(train_prop*length),:].to(torch.float32)\n",
    "corrupted_validation_x = power_corrupted_tensor[int(train_prop*length):int((train_prop+val_prop)*length)+1,:].to(torch.float32)\n",
    "corrupted_test_x = power_corrupted_tensor[int((train_prop+val_prop)*length)+1:length,:].to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd75a942-0388-4404-9318-aaea96da7b48",
   "metadata": {
    "id": "bd75a942-0388-4404-9318-aaea96da7b48"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "else torch.device(\"cpu\")\n",
    "\n",
    "x_train, y_train = data_transform(train_x.numpy(), corrupted_train_x.numpy(), n_his, device)\n",
    "x_val, y_val = data_transform(validation_x.numpy(), corrupted_validation_x.numpy(), n_his, device)\n",
    "x_test, y_test = data_transform(test_x.numpy(), corrupted_test_x.numpy(), n_his, device)\n",
    "\n",
    "# create torch data iterables for training\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(test_data, batch_size)\n",
    "\n",
    "# format graph for pyg layer inputs\n",
    "G = sp.coo_matrix(W)\n",
    "edge_index = torch.tensor(np.array([G.row, G.col]), dtype=torch.int64).to(device)\n",
    "edge_weight = torch.tensor(G.data).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c78ceb-fd9f-4b0f-bd06-09ec43b67f04",
   "metadata": {
    "id": "c9c78ceb-fd9f-4b0f-bd06-09ec43b67f04"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f59c4e1-c3d7-4b9a-ad72-8f56485c16c4",
   "metadata": {
    "id": "6f59c4e1-c3d7-4b9a-ad72-8f56485c16c4"
   },
   "outputs": [],
   "source": [
    "model = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True).to(device)\n",
    "# define loss function\n",
    "loss = nn.MSELoss()\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380158a-750f-4644-aadc-405818b3a385",
   "metadata": {
    "id": "4380158a-750f-4644-aadc-405818b3a385",
    "outputId": "36fa8dee-9764-4614-aaf5-df7efb1fb66f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:   2%|▏         | 1/50 [01:54<1:33:27, 114.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 321.651522954305 \t\t Validation Loss: 86.71108996073404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:   4%|▍         | 2/50 [03:48<1:31:24, 114.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t\t Training Loss: 66.79544210036596 \t\t Validation Loss: 41.32528076171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:37<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:   6%|▌         | 3/50 [05:43<1:29:53, 114.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t\t Training Loss: 51.87682774066925 \t\t Validation Loss: 35.65103982289632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s]\n",
      "Epoch:   8%|▊         | 4/50 [07:38<1:28:01, 114.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t\t Training Loss: 46.30594969590505 \t\t Validation Loss: 34.77217693328858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  10%|█         | 5/50 [09:33<1:25:57, 114.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t\t Training Loss: 42.58097353776296 \t\t Validation Loss: 32.21418863932292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  12%|█▏        | 6/50 [11:27<1:23:58, 114.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 \t\t Training Loss: 39.60454428990682 \t\t Validation Loss: 22.616121514638266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  14%|█▍        | 7/50 [13:21<1:22:03, 114.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t\t Training Loss: 36.81335618893306 \t\t Validation Loss: 21.184365240732827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  16%|█▌        | 8/50 [15:15<1:20:02, 114.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 \t\t Training Loss: 35.03980387846629 \t\t Validation Loss: 19.023947334289552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:35<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  18%|█▊        | 9/50 [17:09<1:18:01, 114.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 \t\t Training Loss: 34.67528150876363 \t\t Validation Loss: 23.020821221669515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:35<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:  20%|██        | 10/50 [19:03<1:16:06, 114.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t\t Training Loss: 32.40992983977 \t\t Validation Loss: 20.18441168467204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:37<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:  22%|██▏       | 11/50 [20:59<1:14:25, 114.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 \t\t Training Loss: 30.426314838727315 \t\t Validation Loss: 16.935174210866293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:37<00:00,  1.23it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:  24%|██▍       | 12/50 [22:54<1:12:41, 114.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 \t\t Training Loss: 29.76183337767919 \t\t Validation Loss: 16.96241108576457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:37<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s]\n",
      "Epoch:  26%|██▌       | 13/50 [24:49<1:10:51, 114.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 \t\t Training Loss: 29.408819429079692 \t\t Validation Loss: 16.51108242670695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  28%|██▊       | 14/50 [26:44<1:08:56, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 \t\t Training Loss: 28.472656619548797 \t\t Validation Loss: 12.885251267751057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  30%|███       | 15/50 [28:39<1:06:57, 114.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t\t Training Loss: 28.390582219759622 \t\t Validation Loss: 16.564135837554932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  32%|███▏      | 16/50 [30:33<1:04:59, 114.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 \t\t Training Loss: 27.813238229354223 \t\t Validation Loss: 18.772869396209718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  34%|███▍      | 17/50 [32:27<1:03:01, 114.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 \t\t Training Loss: 27.46793311436971 \t\t Validation Loss: 14.605381043752034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.67it/s]\n",
      "Epoch:  36%|███▌      | 18/50 [34:22<1:01:04, 114.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 \t\t Training Loss: 27.03142478863398 \t\t Validation Loss: 14.953581301371257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  38%|███▊      | 19/50 [36:16<59:08, 114.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 \t\t Training Loss: 26.85629510084788 \t\t Validation Loss: 13.34765723546346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  40%|████      | 20/50 [38:10<57:12, 114.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t\t Training Loss: 27.218459550539652 \t\t Validation Loss: 15.285479545593262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  42%|████▏     | 21/50 [40:05<55:15, 114.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 \t\t Training Loss: 26.680081395308175 \t\t Validation Loss: 14.533319250742595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  44%|████▍     | 22/50 [41:59<53:19, 114.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 \t\t Training Loss: 26.416524453957877 \t\t Validation Loss: 12.369440937042237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  46%|████▌     | 23/50 [43:53<51:23, 114.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 \t\t Training Loss: 26.90916500091553 \t\t Validation Loss: 11.193509896596273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  48%|████▊     | 24/50 [45:47<49:27, 114.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 \t\t Training Loss: 26.06408806244532 \t\t Validation Loss: 12.226082928975423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  50%|█████     | 25/50 [47:41<47:31, 114.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 \t\t Training Loss: 26.17925492525101 \t\t Validation Loss: 11.252468570073445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  52%|█████▏    | 26/50 [49:35<45:38, 114.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 \t\t Training Loss: 26.264527467886605 \t\t Validation Loss: 15.133753681182862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  54%|█████▍    | 27/50 [51:29<43:44, 114.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 \t\t Training Loss: 26.161232797304788 \t\t Validation Loss: 12.854628117879232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  56%|█████▌    | 28/50 [53:23<41:50, 114.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 \t\t Training Loss: 25.529365452130637 \t\t Validation Loss: 11.82730614344279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  58%|█████▊    | 29/50 [55:17<39:56, 114.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 \t\t Training Loss: 25.527288967370986 \t\t Validation Loss: 14.266114362080891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  60%|██████    | 30/50 [57:11<38:01, 114.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 \t\t Training Loss: 25.451970553398134 \t\t Validation Loss: 15.695348644256592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.69it/s]\n",
      "Epoch:  62%|██████▏   | 31/50 [59:05<36:07, 114.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 \t\t Training Loss: 25.102936283747354 \t\t Validation Loss: 12.333730204900105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:  64%|██████▍   | 32/50 [1:01:00<34:17, 114.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 \t\t Training Loss: 25.31022694905599 \t\t Validation Loss: 13.373649835586548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:37<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  66%|██████▌   | 33/50 [1:02:55<32:26, 114.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 \t\t Training Loss: 25.430078867077828 \t\t Validation Loss: 15.19635992050171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  68%|██████▊   | 34/50 [1:04:49<30:30, 114.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 \t\t Training Loss: 25.390781863530478 \t\t Validation Loss: 13.436627292633057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:35<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.69it/s]\n",
      "Epoch:  70%|███████   | 35/50 [1:06:43<28:32, 114.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 \t\t Training Loss: 25.2624924103419 \t\t Validation Loss: 10.430302381515503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  72%|███████▏  | 36/50 [1:08:37<26:38, 114.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 \t\t Training Loss: 24.58700619141261 \t\t Validation Loss: 9.326819515228271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "Epoch:  74%|███████▍  | 37/50 [1:10:32<24:46, 114.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 \t\t Training Loss: 24.97431607047717 \t\t Validation Loss: 11.018994013468424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  76%|███████▌  | 38/50 [1:12:26<22:52, 114.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 \t\t Training Loss: 24.69193421403567 \t\t Validation Loss: 20.54003143310547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.24it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  78%|███████▊  | 39/50 [1:14:20<20:57, 114.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 \t\t Training Loss: 24.578766798973085 \t\t Validation Loss: 9.996404186884563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  80%|████████  | 40/50 [1:16:15<19:02, 114.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 \t\t Training Loss: 24.547797735532125 \t\t Validation Loss: 12.74273935953776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  82%|████████▏ | 41/50 [1:18:09<17:07, 114.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 \t\t Training Loss: 24.337539593378704 \t\t Validation Loss: 10.499954493840535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  84%|████████▍ | 42/50 [1:20:03<15:13, 114.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 \t\t Training Loss: 24.59947317838669 \t\t Validation Loss: 10.900748475392659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  86%|████████▌ | 43/50 [1:21:57<13:19, 114.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 \t\t Training Loss: 23.711292986075083 \t\t Validation Loss: 11.032711394627889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  88%|████████▊ | 44/50 [1:23:51<11:24, 114.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 \t\t Training Loss: 23.850187496344248 \t\t Validation Loss: 11.933543745676676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.67it/s]\n",
      "Epoch:  90%|█████████ | 45/50 [1:25:45<09:30, 114.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 \t\t Training Loss: 23.70346832672755 \t\t Validation Loss: 9.121953582763672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 120/120 [01:36<00:00,  1.25it/s]\n",
      "Batch: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
      "Epoch:  94%|█████████▍| 47/50 [1:29:34<05:42, 114.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 \t\t Training Loss: 23.773285833994546 \t\t Validation Loss: 10.436311308542887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  58%|█████▊    | 70/120 [00:56<00:39,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs + 1), desc = 'Epoch', position = 0):\n",
    "    train_loss, n = 0.0, 0\n",
    "    model.train()\n",
    "    \n",
    "    for x, y in tqdm(train_iter, desc = 'Batch', position = 0):\n",
    "        # get model predictions and compute loss\n",
    "        y_pred = model(x.to(device), edge_index, edge_weight)\n",
    "        loss = torch.mean((y_pred-y)**2)\n",
    "        # backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    model.eval() \n",
    "    for x, y in tqdm(val_iter, desc = 'Batch', position = 0):\n",
    "        # get model predictions and compute loss\n",
    "        y_pred = model(x.to(device), edge_index, edge_weight)\n",
    "        loss = torch.mean((y_pred-y)**2)\n",
    "        valid_loss += loss.item() \n",
    "\n",
    "    print(f'Epoch {epoch} \\t\\t Training Loss: {train_loss/120} \\t\\t Validation Loss: {valid_loss/30}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa8a43-e106-4c9c-88ef-bb5e400107f4",
   "metadata": {
    "id": "0baa8a43-e106-4c9c-88ef-bb5e400107f4"
   },
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d761fee-d8fa-411f-94fb-9aec70b9fe1c",
   "metadata": {
    "id": "7d761fee-d8fa-411f-94fb-9aec70b9fe1c",
    "outputId": "6c457037-23d4-4ea9-8e2c-675148c9e3a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 30/30 [00:23<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 288, 35, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load model with lowest validation lost\n",
    "best_model = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True).to(device)\n",
    "best_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "best_model.eval()\n",
    "cost = 0\n",
    "missing_count = 0\n",
    "predicted = []\n",
    "ground_truth = []\n",
    "\n",
    "i = 1\n",
    "\n",
    "for x, y in tqdm(test_iter, desc = 'Batch', position = 0):\n",
    "    # get model predictions and compute loss\n",
    "    y_pred = best_model(x.to(device), edge_index, edge_weight)\n",
    "    if i == 1:\n",
    "        y_pred_complete = y_pred\n",
    "    else:\n",
    "        y_pred_complete = torch.cat((y_pred_complete, y_pred), 0)\n",
    "    i+=1\n",
    "\n",
    "print(y_pred_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948464d1-d3c0-4384-86ba-901e2ceba668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of STGCN-DAE is: 4.673917496141987\n",
      "Test MAE is of STGCN-DAE is: tensor(1.8913, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "pred = y_pred_complete[x_test.cpu().numpy()==-1]\n",
    "ground_truth = y_test[x_test.cpu().numpy()==-1]\n",
    "print(\"Test RMSE of STGCN-DAE is: \"+ str(sqrt(torch.mean((pred-ground_truth)**2))))\n",
    "print(\"Test MAE is of STGCN-DAE is: \"+ str(torch.mean(abs(pred-ground_truth))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CmahC9sDtdNe",
   "metadata": {
    "id": "CmahC9sDtdNe"
   },
   "source": [
    "#Baseline\n",
    "Only four baselines are here. Scripts for baselines MIDA and LRTC-TNN are ran separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd893f-92f5-4a9b-b62f-55755a380ce9",
   "metadata": {
    "id": "17dd893f-92f5-4a9b-b62f-55755a380ce9"
   },
   "source": [
    "LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84a33c97-3209-4d91-b3d9-8a7e6830b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of Linear Interpolation is: 19.225026763381486\n",
      "Test MAE of Linear Interpolation is: tensor(14.9283)\n"
     ]
    }
   ],
   "source": [
    "#Linear Interpolation\n",
    "corrupted_test_x[corrupted_test_x==-1] = np.nan\n",
    "test = pd.DataFrame(corrupted_test_x.numpy())\n",
    "LI_imputed = test.interpolate(method ='linear', limit_direction ='forward')\n",
    "#LI_imputed = LI_imputed.dropna()\n",
    "LI_imputed = LI_imputed.fillna(LI_imputed.mean())\n",
    "LI_imputed = torch.tensor(LI_imputed.values)\n",
    "LI_pred = LI_imputed[~mask[86400:103680,]]\n",
    "print(\"Test RMSE of Linear Interpolation is: \"+ str(sqrt(torch.mean((LI_pred.cpu()-ground_truth.cpu())**2))))\n",
    "print(\"Test MAE of Linear Interpolation is: \"+ str(torch.mean(abs(LI_pred.cpu()-ground_truth.cpu()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083147e5-7788-489f-ba58-67b04b72486f",
   "metadata": {
    "id": "083147e5-7788-489f-ba58-67b04b72486f"
   },
   "source": [
    "Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c357bb3-7a03-4a90-9382-2f3f8a649c4c",
   "metadata": {
    "id": "7c357bb3-7a03-4a90-9382-2f3f8a649c4c",
    "outputId": "b8cf4d31-536e-4d83-a54d-141773a00e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of Mean Imputation is: 28.15015910291683\n",
      "Test MAE of Mean Imputation is: tensor(26.4180)\n"
     ]
    }
   ],
   "source": [
    "#Mean Imputation\n",
    "from math import sqrt\n",
    "corrupted_test_x[corrupted_test_x==-1] = np.nan\n",
    "test = pd.DataFrame(corrupted_test_x.numpy())\n",
    "Mean_imputed = test.fillna(test.mean())\n",
    "Mean_imputed = torch.tensor(Mean_imputed.values)\n",
    "Mean_pred = Mean_imputed[~mask[86400:103680,]]\n",
    "print(\"Test RMSE of Mean Imputation is: \"+ str(sqrt(torch.mean((Mean_pred.cpu()-ground_truth.cpu())**2))))\n",
    "print(\"Test MAE of Mean Imputation is: \"+ str(torch.mean(abs(Mean_pred.cpu()-ground_truth.cpu()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f71abe-28e0-42c5-97b7-b35f8c18a24d",
   "metadata": {
    "id": "02f71abe-28e0-42c5-97b7-b35f8c18a24d"
   },
   "source": [
    "MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e8c69-e869-4bc2-bb5f-716477b1a692",
   "metadata": {
    "id": "529e8c69-e869-4bc2-bb5f-716477b1a692",
    "outputId": "e92daea1-0765-44ac-a3e8-84345a4a2a4b"
   },
   "outputs": [],
   "source": [
    "#MICE\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "imp = IterativeImputer(estimator = knn, max_iter = 1, initial_strategy = 'median', imputation_order='ascending',random_state=42)\n",
    "corrupted_train_x[corrupted_train_x==-1] = np.nan\n",
    "train = pd.DataFrame(corrupted_train_x.numpy())\n",
    "imp.fit(train)\n",
    "corrupted_test_x[corrupted_test_x==-1] = np.nan\n",
    "test = pd.DataFrame(corrupted_test_x.numpy())\n",
    "MICE_imputed = imp.transform(test)\n",
    "MICE_imputed = pd.DataFrame(MICE_imputed)\n",
    "MICE_imputed = torch.tensor(MICE_imputed.values)\n",
    "MICE_pred = MICE_imputed[~mask[86400:103680,]]\n",
    "ground_truth = y_test[x_test.cpu().numpy()==-1]\n",
    "print(\"Test RMSE of MICE Interpolation is: \"+ str(sqrt(torch.mean((MICE_pred.cpu()-ground_truth.cpu())**2))))\n",
    "print(\"Test MAE of MICE Interpolation is: \"+ str(torch.mean(abs(MICE_pred.cpu()-ground_truth.cpu()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e61f31-3c13-4e93-8d81-e78b1519977e",
   "metadata": {
    "id": "37e61f31-3c13-4e93-8d81-e78b1519977e"
   },
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a3cb3a0-30f3-4149-a1ef-c0ea886e4a5e",
   "metadata": {
    "id": "7a3cb3a0-30f3-4149-a1ef-c0ea886e4a5e",
    "outputId": "e7965947-64dc-4c8c-86ba-3662737b50aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of KNN Interpolation is: 11.31404432008966\n",
      "Test MAE of KNN Interpolation is: tensor(5.6011)\n"
     ]
    }
   ],
   "source": [
    "# KNN Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "corrupted_train_x[corrupted_train_x==-1] = np.nan\n",
    "train = pd.DataFrame(corrupted_train_x.numpy())\n",
    "imputer.fit(train)\n",
    "KNN_imputed = imputer.transform(test)\n",
    "KNN_imputed = pd.DataFrame(KNN_imputed)\n",
    "KNN_imputed = torch.tensor(KNN_imputed.values)\n",
    "KNN_pred = KNN_imputed[~mask[86400:103680,]]\n",
    "print(\"Test RMSE of KNN Interpolation is: \"+ str(sqrt(torch.mean((KNN_pred.cpu()-ground_truth.cpu())**2))))\n",
    "print(\"Test MAE of KNN Interpolation is: \"+ str(torch.mean(abs(KNN_pred.cpu()-ground_truth.cpu()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81404191-f0ea-4621-b622-b40a018fea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STD-GAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
